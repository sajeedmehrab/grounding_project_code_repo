{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88803c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ksmehrab/miniconda/envs/glamm/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be98b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksmehrab/miniconda/envs/glamm/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef956a0e",
   "metadata": {},
   "source": [
    "## Setup and main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd23d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m---- Initialized tokenizer from: MBZUAI/GLaMM-RefSeg ----\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m---- Initialized model from: MBZUAI/GLaMM-RefSeg ----\u001b[0m\n",
      "\u001b[92m---- Initialized Global Image Encoder (vision tower) from: openai/clip-vit-large-patch14-336 ----\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14-336 were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'logit_scale', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'visual_projection.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias']\n",
      "- This IS expected if you are initializing CLIPVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 37000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 13.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ksmehrab/AttentionGrounding/Baselines/Models/GLAMM')\n",
    "sys.path.append('/home/ksmehrab/AttentionGrounding/Baselines/Datasets')\n",
    "\n",
    "from run_glamm import segment_image_using_glamm\n",
    "from pascalpart import get_pascalpart_classes\n",
    "from utils import read_txt_file, save_to_json\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8857ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already done on 0 images. Running on remaining 925...\n"
     ]
    }
   ],
   "source": [
    "pascal_image_dir = \"/data/Pascal_VOC_2012/VOCdevkit/VOC2012/JPEGImages\" # 17125 images\n",
    "annotations_path= \"/data/PartSegmentationDatasets/PascalPart/Annotations_Part\"\n",
    "val_filepath = \"/data/PartSegmentationDatasets/PascalPart/val.txt\" # 925 images. File contains just the file prefix. Add .jpg extension for images, and .mat extension for annotations\n",
    "val_filenames = read_txt_file(val_filepath)\n",
    "\n",
    "model_name = 'glamm'\n",
    "\n",
    "# This is where crop results are saved \n",
    "base_crop_save_dir = \"/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM\"\n",
    "if not os.path.exists(base_crop_save_dir):\n",
    "    os.makedirs(base_crop_save_dir)\n",
    "\n",
    "# save_dir is where results of initial segmentations have been saved \n",
    "save_dir = \"/data/VLMGroundingProject/BaselineResults/PascalPart/GLAMM\"\n",
    "if not os.path.exists(save_dir):\n",
    "    raise ValueError(f\"save_dir {save_dir} does not exist. Run the segmentation first.\")\n",
    "\n",
    "already_done = [os.path.splitext(f)[0] for f in os.listdir(base_crop_save_dir)]\n",
    "\n",
    "req_filenames = [f for f in val_filenames if f not in already_done]\n",
    "\n",
    "print(f'Already done on {len(already_done)} images. Running on remaining {len(req_filenames)}...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b720699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_from_mask(mask_path):\n",
    "    \"\"\"Extract bounding box from a binary mask.\"\"\"\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    coords = np.where(mask > 0)\n",
    "    if len(coords[0]) == 0:\n",
    "        return None\n",
    "    \n",
    "    y_min, y_max = coords[0].min(), coords[0].max()\n",
    "    x_min, x_max = coords[1].min(), coords[1].max()\n",
    "    \n",
    "    # Add some padding\n",
    "    padding = 10\n",
    "    return [max(0, x_min-padding), max(0, y_min-padding), \n",
    "            x_max+padding, y_max+padding]\n",
    "\n",
    "def crop_image_to_bbox(image_path, bbox):\n",
    "    \"\"\"Crop image to bounding box.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    return image.crop(bbox)\n",
    "\n",
    "def run_part_segmentation_on_crops():\n",
    "    PROMPT_TEMPLATE = \"Can you segment the <obj> in this image?\"\n",
    "    \n",
    "    for filename in tqdm(req_filenames):\n",
    "        img_filepath = os.path.join(pascal_image_dir, filename+'.jpg')\n",
    "        annot_filename = filename + '.mat'\n",
    "        classes_to_detect = get_pascalpart_classes(annot_filename, annotations_path)\n",
    "\n",
    "        save_dir_for_example = os.path.join(save_dir, filename)\n",
    "        \n",
    "        crop_save_dir = os.path.join(base_crop_save_dir, filename)\n",
    "        os.makedirs(crop_save_dir, exist_ok=True)\n",
    "        \n",
    "        for object, parts in classes_to_detect.items():\n",
    "            object_underscore = object.replace(' ', '_')\n",
    "            object_mask_path = os.path.join(save_dir_for_example, f'{object_underscore}.png')\n",
    "            \n",
    "            if not os.path.exists(object_mask_path):\n",
    "                continue\n",
    "                \n",
    "            # Get bbox from object mask\n",
    "            bbox = get_bbox_from_mask(object_mask_path)\n",
    "            if bbox is None:\n",
    "                continue\n",
    "                \n",
    "            # Crop the original image\n",
    "            cropped_image = crop_image_to_bbox(img_filepath, bbox)\n",
    "            cropped_image_path = os.path.join(crop_save_dir, f'{object_underscore}_crop.jpg')\n",
    "            cropped_image.save(cropped_image_path)\n",
    "            \n",
    "            # Segment parts in the cropped image\n",
    "            for part in parts:\n",
    "                part_prompt = PROMPT_TEMPLATE.replace('<obj>', f'{object} {part}')\n",
    "                \n",
    "                part_mask_crop = segment_image_using_glamm(\n",
    "                    input_image_path=cropped_image_path,\n",
    "                    prompt_text=part_prompt\n",
    "                )\n",
    "                \n",
    "                part_underscore = part.replace(' ', '_')\n",
    "                part_mask_crop.save(os.path.join(crop_save_dir, f'{object_underscore}_{part_underscore}_crop.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf8d941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 202/925 [26:07<1:33:29,  7.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_part_segmentation_on_crops\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m, in \u001b[0;36mrun_part_segmentation_on_crops\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts:\n\u001b[1;32m     53\u001b[0m     part_prompt \u001b[38;5;241m=\u001b[39m PROMPT_TEMPLATE\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<obj>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mobject\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m     part_mask_crop \u001b[38;5;241m=\u001b[39m \u001b[43msegment_image_using_glamm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_image_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcropped_image_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpart_prompt\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     part_underscore \u001b[38;5;241m=\u001b[39m part\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m     part_mask_crop\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(crop_save_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_underscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpart_underscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_crop.png\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/AttentionGrounding/Baselines/Models/GLAMM/run_glamm.py:48\u001b[0m, in \u001b[0;36msegment_image_using_glamm\u001b[0;34m(input_image_path, prompt_text)\u001b[0m\n\u001b[1;32m     45\u001b[0m generate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m output_image, markdown_out, seg_mask \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_up\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m seg_mask\n",
      "File \u001b[0;32m~/GLAMM/groundingLMM/app_modified.py:250\u001b[0m, in \u001b[0;36minference\u001b[0;34m(input_str, all_inputs, follow_up, generate, verbose, return_attentions)\u001b[0m\n\u001b[1;32m    247\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Pass prepared inputs to model\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m output_ids, pred_masks, attentions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_enc_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrounding_enc_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_size_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens_new\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# breakpoint in inference function. check output_ids, pred_masks, attentions\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# breakpoint()\u001b[39;00m\n\u001b[1;32m    256\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m output_ids[\u001b[38;5;241m0\u001b[39m][output_ids[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m IMAGE_TOKEN_INDEX]\n",
      "File \u001b[0;32m~/GLAMM/groundingLMM/model/GLaMM.py:300\u001b[0m, in \u001b[0;36mGLaMMForCausalLM.evaluate\u001b[0;34m(self, global_enc_images, grounding_enc_images, input_ids, resize_list, orig_sizes, max_tokens_new, bboxes, return_attentions)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    290\u001b[0m         global_enc_images, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         return_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    298\u001b[0m ):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 300\u001b[0m         generation_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_enc_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens_new\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attentions\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;66;03m# breakpoint in evaluate function of GLaMMForCausalLM. check generation_outputs and its keys\u001b[39;00m\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;66;03m# breakpoint()\u001b[39;00m\n\u001b[1;32m    314\u001b[0m         output_hidden_states \u001b[38;5;241m=\u001b[39m generation_outputs\u001b[38;5;241m.\u001b[39mhidden_states\n",
      "File \u001b[0;32m~/miniconda/envs/glamm/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/glamm/lib/python3.10/site-packages/transformers/generation/utils.py:1416\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1411\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when doing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1412\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m greedy search.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1413\u001b[0m         )\n\u001b[1;32m   1415\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/envs/glamm/lib/python3.10/site-packages/transformers/generation/utils.py:2211\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2208\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2210\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2211\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2214\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2215\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2219\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GLAMM/groundingLMM/model/GLaMM.py:131\u001b[0m, in \u001b[0;36mGLaMMForCausalLM.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/GLAMM/groundingLMM/model/llava/model/language_model/llava_llama.py:92\u001b[0m, in \u001b[0;36mLlavaLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, bboxes, return_dict)\u001b[0m\n\u001b[1;32m     77\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     78\u001b[0m     output_hidden_states\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     82\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     83\u001b[0m     return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m     84\u001b[0m )\n\u001b[1;32m     86\u001b[0m (\n\u001b[1;32m     87\u001b[0m     input_ids,\n\u001b[1;32m     88\u001b[0m     attention_mask,\n\u001b[1;32m     89\u001b[0m     past_key_values,\n\u001b[1;32m     90\u001b[0m     inputs_embeds,\n\u001b[1;32m     91\u001b[0m     labels,\n\u001b[0;32m---> 92\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_inputs_labels_for_multimodal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m     97\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m     98\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m     99\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    106\u001b[0m )\n",
      "File \u001b[0;32m~/GLAMM/groundingLMM/model/llava/llava_with_region_arch.py:109\u001b[0m, in \u001b[0;36mLlavaMetaForCausalLM.prepare_inputs_labels_for_multimodal\u001b[0;34m(self, input_ids, attention_mask, past_key_values, labels, images, bboxes)\u001b[0m\n\u001b[1;32m    106\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m image_features]\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# Process for region\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     image_features, image_forward_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mwith_region:\n\u001b[1;32m    111\u001b[0m         select_hidden_state_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmm_vision_select_layer\n",
      "File \u001b[0;32m~/GLAMM/groundingLMM/model/llava/llava_with_region_arch.py:79\u001b[0m, in \u001b[0;36mLlavaMetaForCausalLM.encode_images\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_images\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[0;32m---> 79\u001b[0m     image_features, image_forward_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vision_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39mmm_projector(image_features)\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_features, image_forward_outs\n",
      "File \u001b[0;32m~/miniconda/envs/glamm/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/glamm/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GLAMM/groundingLMM/model/llava/model/multimodal_encoder/clip_encoder.py:60\u001b[0m, in \u001b[0;36mCLIPVisionTower.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     54\u001b[0m     image_forward_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_tower(\n\u001b[1;32m     55\u001b[0m         images\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype),\n\u001b[1;32m     56\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_select(image_forward_outs)\u001b[38;5;241m.\u001b[39mto(images\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m---> 60\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_region:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_features, image_forward_outs\n",
      "File \u001b[0;32m~/miniconda/envs/glamm/lib/python3.10/site-packages/torch/cuda/memory.py:125\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 125\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_part_segmentation_on_crops()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa9f08",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7916887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ksmehrab/AttentionGrounding/Baselines/Datasets')\n",
    "sys.path.append('/home/ksmehrab/AttentionGrounding/Baselines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4da39fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pascalpart import get_pascalpart_masks\n",
    "from utils import read_txt_file\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32bf2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "pascal_image_dir = \"/data/Pascal_VOC_2012/VOCdevkit/VOC2012/JPEGImages\" # 17125 images\n",
    "annotations_path= \"/data/PartSegmentationDatasets/PascalPart/Annotations_Part\"\n",
    "val_filepath = \"/data/PartSegmentationDatasets/PascalPart/val.txt\" # 925 images. File contains just the file prefix. Add .jpg extension for images, and .mat extension for annotations\n",
    "base_pred_dir = \"/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM\"\n",
    "orig_pred_dir = \"/data/VLMGroundingProject/BaselineResults/PascalPart/GLAMM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12e9a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_filenames = os.listdir(base_pred_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aef53134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_masks(masks):\n",
    "    combined_mask = masks[0]\n",
    "    for i in range(1, len(masks)):\n",
    "        combined_mask = combined_mask | masks[i]\n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76c4b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainercv.evaluations import calc_semantic_segmentation_confusion\n",
    "def compute_miou(preds, gts):\n",
    "    '''\n",
    "    This function takes two lists as inputs, where:\n",
    "    preds: [list of 2-D prediction where each pixel corresponds to pixel-label]\n",
    "    gts: [list of ground-truths in 2-D space, contains -1 value for critical pixels]\n",
    "    \n",
    "    returns mean of the iou for all classes\n",
    "    '''\n",
    "    confusion = calc_semantic_segmentation_confusion(preds, gts)\n",
    "\n",
    "    gtj = confusion.sum(axis=1)\n",
    "    resj = confusion.sum(axis=0)\n",
    "    gtjresj = np.diag(confusion)\n",
    "    denominator = gtj + resj - gtjresj\n",
    "    iou = gtjresj / denominator\n",
    "\n",
    "    return iou, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae643fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glamm_predicted_mask(base_pred_dir, filename_prefix, class_name, is_crop=False):\n",
    "    \"\"\"\n",
    "        Get the predicted mask for a given image and class from GLAMM predictions.\n",
    "        Args:\n",
    "            base_pred_dir (str): Base directory where GLAMM predictions are stored.\n",
    "            filename_prefix (str): Prefix of the filename (without extension).\n",
    "            class_name (str): Name of the class for which the mask is required.\n",
    "    \"\"\"\n",
    "    class_name = class_name.replace(\" \", \"_\")\n",
    "    # all pred masks for this filename is in a directory named filename_prefix\n",
    "    pred_masks_dir = os.path.join(base_pred_dir, filename_prefix)\n",
    "    # the required mask is in a file named class_name.png. here, class_name can either be the object class (like person) or the object || part class (like person_head)\n",
    "    if not is_crop:\n",
    "        req_pred_mask = os.path.join(pred_masks_dir, class_name + \".png\")\n",
    "    else:\n",
    "        req_pred_mask = os.path.join(pred_masks_dir, class_name+'_crop' + \".png\")\n",
    "    # Load the mask\n",
    "    mask = Image.open(req_pred_mask).convert(\"L\")  # grayscale\n",
    "\n",
    "    # Convert to numpy, then 0/1\n",
    "    mask_np = np.array(mask)\n",
    "    binary_mask = (mask_np > 0).astype(np.uint8)\n",
    "    \n",
    "    return binary_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6eb35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_crop_mask_to_full_image(crop_mask_path, bbox, original_image_shape):\n",
    "    \"\"\"Transform a mask from crop coordinates back to full image coordinates.\"\"\"\n",
    "    crop_mask = cv2.imread(crop_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if crop_mask is None:\n",
    "        print(\"Crop mask not found at:\", crop_mask_path)\n",
    "        return None\n",
    "    \n",
    "    # Create full-size mask\n",
    "    full_mask = np.zeros(original_image_shape[:2], dtype=np.uint8)\n",
    "    \n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    crop_h, crop_w = crop_mask.shape\n",
    "    \n",
    "    # Handle potential size mismatches\n",
    "    actual_h = min(crop_h, y_max - y_min, original_image_shape[0] - y_min)\n",
    "    actual_w = min(crop_w, x_max - x_min, original_image_shape[1] - x_min)\n",
    "    \n",
    "    # Place the crop mask in the full image\n",
    "    full_mask[y_min:y_min+actual_h, x_min:x_min+actual_w] = crop_mask[:actual_h, :actual_w]\n",
    "    \n",
    "    return full_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dea35039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/203 [00:00<?, ?it/s][ WARN:0@278.885] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000811/person_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@278.898] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004072/bus_front side_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@278.910] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001992/person_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@278.920] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003105/train_head front side_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@278.929] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003904/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@278.940] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003928/dog_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@278.953] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001082/bus_front side_crop.png'): can't open/read file: check file path/integrity\n",
      "  4%|▍         | 8/203 [00:00<00:02, 76.33it/s][ WARN:0@278.962] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001874/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@278.971] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002043/bird_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@278.984] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000992/person_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@278.999] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001008/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.011] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002904/dog_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.018] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000845/train_head front side_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.027] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001966/bird_right eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.036] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003071/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.048] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003991/person_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.053] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004021/bird_right eye_crop.png'): can't open/read file: check file path/integrity\n",
      "  9%|▉         | 19/203 [00:00<00:01, 92.60it/s][ WARN:0@279.068] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000156/dog_left ear_crop.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000811/person_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004072/bus_front side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001992/person_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003105/train_head front side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003904/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003928/dog_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001082/bus_front side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001874/person_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002043/bird_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000992/person_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001008/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002904/dog_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000845/train_head front side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001966/bird_right eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003071/person_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003991/person_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004021/bird_right eye_crop.png\n",
      "no parts present in 2009_000087, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000156/dog_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000123/bird_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001895/dog_right ear_crop.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@279.077] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000123/bird_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.085] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001895/dog_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.101] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003874/person_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.110] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002155/dog_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.123] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004848/person_left lower arm_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.130] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004069/car_front side_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.144] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004140/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.149] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004175/horse_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.157] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002035/cow_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.167] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001108/bird_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      " 15%|█▌        | 31/203 [00:00<00:01, 97.22it/s][ WARN:0@279.186] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002835/horse_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.200] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003196/motorbike_front wheel_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.209] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003849/person_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.221] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002152/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.233] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002185/person_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "/tmp/ipykernel_1669812/4210323992.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  iou = gtjresj / denominator\n",
      "[ WARN:0@279.249] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004099/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.255] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001885/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.267] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002864/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.276] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002936/dog_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      " 21%|██        | 42/203 [00:00<00:01, 101.38it/s][ WARN:0@279.287] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002165/horse_left eye_crop.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no parts present in 2008_003858, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003874/person_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002155/dog_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004848/person_left lower arm_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004069/car_front side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004140/person_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004175/horse_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002035/cow_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001108/bird_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002835/horse_left ear_crop.png\n",
      "no parts present in 2008_001040, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003196/motorbike_front wheel_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003849/person_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002152/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002185/person_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004099/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001885/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002864/person_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002936/dog_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002165/horse_left eye_crop.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@279.295] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004033/dog_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.306] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005038/person_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.314] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003155/aeroplane_left wing_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.322] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000863/dog_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.332] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005156/bicycle_front wheel_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.337] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005189/aeroplane_left wing_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.349] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002929/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.366] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002122/cow_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.380] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002888/aeroplane_left wing_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.386] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003876/aeroplane_left wing_crop.png'): can't open/read file: check file path/integrity\n",
      " 27%|██▋       | 55/203 [00:00<00:01, 110.09it/s][ WARN:0@279.399] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000037/bird_right eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.409] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002042/person_left lower arm_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.418] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002171/cow_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.426] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003856/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.435] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001074/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.459] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003105/train_head right side_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.467] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001028/person_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.473] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000039/sheep_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.479] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000919/cow_left back lower leg_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.486] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005078/motorbike_front wheel_crop.png'): can't open/read file: check file path/integrity\n",
      " 33%|███▎      | 68/203 [00:00<00:01, 115.35it/s][ WARN:0@279.494] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002982/train_coach left side_crop.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004033/dog_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005038/person_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003155/aeroplane_left wing_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000863/dog_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005156/bicycle_front wheel_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005189/aeroplane_left wing_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002929/person_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002122/cow_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002888/aeroplane_left wing_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003876/aeroplane_left wing_crop.png\n",
      "no parts present in 2009_000032, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000037/bird_right eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002042/person_left lower arm_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002171/cow_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003856/person_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001074/person_right ear_crop.png\n",
      "no parts present in 2008_000120, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003105/train_head right side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001028/person_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000039/sheep_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000919/cow_left back lower leg_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005078/motorbike_front wheel_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002982/train_coach left side_crop.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@279.501] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003043/person_left lower arm_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.509] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005137/bird_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.522] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000931/horse_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.532] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003108/person_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.541] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000080/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.548] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005089/sheep_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.575] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001821/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.587] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004101/bicycle_back wheel_crop.png'): can't open/read file: check file path/integrity\n",
      " 39%|███▉      | 80/203 [00:00<00:01, 116.67it/s][ WARN:0@279.594] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002975/horse_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.613] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000991/bird_right eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.620] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000731/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.628] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000943/dog_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.634] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001070/dog_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.649] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007836/aeroplane_left wing_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.659] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003110/bird_right eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.670] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003782/horse_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.692] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004396/horse_left ear_crop.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003043/person_left lower arm_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005137/bird_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000931/horse_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003108/person_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000080/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005089/sheep_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001821/person_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004101/bicycle_back wheel_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002975/horse_right ear_crop.png\n",
      "no parts present in 2009_000919, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000991/bird_right eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000731/person_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_000943/dog_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001070/dog_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007836/aeroplane_left wing_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003110/bird_right eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003782/horse_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004396/horse_left ear_crop.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 94/203 [00:00<00:00, 121.16it/s][ WARN:0@279.701] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004758/bicycle_front wheel_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.722] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_005089/person_right eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.732] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006063/cow_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.751] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006216/person_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.759] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006553/train_coach left side_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.768] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007031/cow_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.777] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007392/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.784] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003450/bird_right eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.795] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003481/horse_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.807] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003773/cow_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.814] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003858/bird_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.822] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_008421/horse_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      " 53%|█████▎    | 107/203 [00:00<00:00, 114.47it/s][ WARN:0@279.828] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_008746/bird_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.841] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000488/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.850] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000664/bird_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.860] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000716/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.869] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001215/person_right lower arm_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.877] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001255/car_front side_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.888] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_001734/person_right ear_crop.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004758/bicycle_front wheel_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_005089/person_right eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006063/cow_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006216/person_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006553/train_coach left side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007031/cow_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007392/person_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003450/bird_right eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003481/horse_left ear_crop.png\n",
      "no parts present in 2009_003607, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003773/cow_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003858/bird_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_008421/horse_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_008746/bird_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000488/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000664/bird_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000716/cat_left ear_crop.png\n",
      "no parts present in 2009_000732, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001215/person_right lower arm_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001255/car_front side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_001734/person_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001768/person_right lower arm_crop.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@279.902] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001768/person_right lower arm_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.907] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002320/aeroplane_left wing_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.923] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002535/bus_front license plate_crop.png'): can't open/read file: check file path/integrity\n",
      " 59%|█████▉    | 120/203 [00:01<00:00, 116.81it/s][ WARN:0@279.935] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002649/person_right lower arm_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.951] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002936/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.958] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003492/person_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.975] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_004635/cow_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.983] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_004815/person_right lower arm_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@279.994] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_004828/person_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.003] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005174/horse_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.011] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005206/dog_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.019] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005353/bird_right eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.031] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005606/person_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      " 65%|██████▌   | 132/203 [00:01<00:00, 116.88it/s][ WARN:0@280.037] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005644/dog_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.045] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005877/aeroplane_left wing_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.052] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_006054/train_coach left side_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.060] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_001331/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.066] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_001351/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.074] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006055/train_coach left side_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.084] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001076/person_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.093] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001433/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.100] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002205/bus_right side_crop.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002320/aeroplane_left wing_crop.png\n",
      "no parts present in 2009_002415, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002535/bus_front license plate_crop.png\n",
      "no parts present in 2009_002591, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002649/person_right lower arm_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002936/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003492/person_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_004635/cow_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_004815/person_right lower arm_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_004828/person_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005174/horse_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005206/dog_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005353/bird_right eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005606/person_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005644/dog_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005877/aeroplane_left wing_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_006054/train_coach left side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_001331/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_001351/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006055/train_coach left side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001076/person_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_001433/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002205/bus_right side_crop.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@280.127] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002212/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      " 71%|███████   | 144/203 [00:01<00:00, 117.69it/s][ WARN:0@280.138] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002778/cow_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.162] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_004219/motorbike_front wheel_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.171] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_004994/bird_right eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.183] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005344/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.190] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005531/bird_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.199] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005788/sheep_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.211] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000012/horse_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.220] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000318/bird_right wing_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.229] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000335/car_back side_crop.png'): can't open/read file: check file path/integrity\n",
      " 77%|███████▋  | 156/203 [00:01<00:00, 117.33it/s][ WARN:0@280.241] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003733/person_left ear_crop.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002212/person_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_002778/cow_left ear_crop.png\n",
      "no parts present in 2010_004041, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_004219/motorbike_front wheel_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_004994/bird_right eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005344/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005531/bird_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_005788/sheep_left ear_crop.png\n",
      "no parts present in 2010_003531, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000012/horse_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000318/bird_right wing_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000335/car_back side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_003733/person_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004345/motorbike_front wheel_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004399/motorbike_front wheel_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004701/cow_right eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_005445/person_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_005525/horse_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_005633/sheep_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_005727/bicycle_front wheel_crop.png\n",
      "no parts present in 2009_004140, skipping...\n",
      "no parts present in 2009_004217, skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@280.249] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004345/motorbike_front wheel_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.256] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004399/motorbike_front wheel_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.263] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_004701/cow_right eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.290] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_005445/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.298] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_005525/horse_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.306] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_005633/sheep_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.316] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_005727/bicycle_front wheel_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.337] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004504/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      " 83%|████████▎ | 168/203 [00:01<00:00, 115.72it/s][ WARN:0@280.348] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004540/bird_right eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.356] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004748/car_front license plate_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.364] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004987/motorbike_front wheel_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.375] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006325/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.388] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006341/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.395] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006480/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.406] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007498/bird_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.409] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007507/aeroplane_left wing_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.422] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007677/sheep_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.432] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007828/person_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      " 89%|████████▉ | 181/203 [00:01<00:00, 117.58it/s][ WARN:0@280.454] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000964/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004504/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004540/bird_right eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004748/car_front license plate_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_004987/motorbike_front wheel_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006325/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006341/person_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_006480/cat_left ear_crop.png\n",
      "no parts present in 2008_007048, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007498/bird_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007507/aeroplane_left wing_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007677/sheep_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2008_007828/person_right ear_crop.png\n",
      "no parts present in 2008_008103, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_000964/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001300/train_head front side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001816/bus_front license plate_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002221/cow_right ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002346/person_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002568/train_coach right side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002604/bird_left eye_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005158/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_002271/motorbike_front wheel_crop.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@280.464] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001300/train_head front side_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.483] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_001816/bus_front license plate_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.491] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002221/cow_right ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.504] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002346/person_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.512] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002568/train_coach right side_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.524] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_002604/bird_left eye_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.532] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_005158/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.538] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_002271/motorbike_front wheel_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.547] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_002348/cat_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      " 95%|█████████▌| 193/203 [00:01<00:00, 117.46it/s][ WARN:0@280.557] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_002450/bird_left wing_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.567] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_002480/dog_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.588] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_002546/person_left lower arm_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.600] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003003/person_right upper arm_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.608] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003059/cow_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.618] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003065/person_left lower arm_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.627] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003193/bicycle_front wheel_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.634] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003241/dog_left ear_crop.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@280.644] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003387/car_front side_crop.png'): can't open/read file: check file path/integrity\n",
      "100%|██████████| 203/203 [00:01<00:00, 113.32it/s][ WARN:0@280.649] global loadsave.cpp:241 findDecoder imread_('/data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003406/car_back side_crop.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_002348/cat_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_002450/bird_left wing_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_002480/dog_left ear_crop.png\n",
      "no parts present in 2010_002538, skipping...\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2010_002546/person_left lower arm_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003003/person_right upper arm_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003059/cow_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003065/person_left lower arm_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003193/bicycle_front wheel_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003241/dog_left ear_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003387/car_front side_crop.png\n",
      "Crop mask not found at: /data/VLMGroundingProject/BaselineResults/PascalPartCrops/GLAMM/2009_003406/car_back side_crop.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "part_ious = []\n",
    "count_no_obj = 0\n",
    "for filename in tqdm(req_filenames):\n",
    "    # gt_obj_masks = []\n",
    "    # pred_obj_masks = []\n",
    "\n",
    "    gt_part_masks = []\n",
    "    pred_part_masks = []\n",
    "\n",
    "    img_filepath = os.path.join(pascal_image_dir, filename+'.jpg')\n",
    "    original_image = cv2.imread(img_filepath)\n",
    "    original_shape = original_image.shape\n",
    "\n",
    "    annot_filename = filename + '.mat'\n",
    "    anno_dict = get_pascalpart_masks(annot_filename, annotations_path, images_path=pascal_image_dir)\n",
    "    orig_pred_dir_for_example = os.path.join(orig_pred_dir, filename)\n",
    "    \n",
    "    for obj_name, anno in anno_dict.items():\n",
    "        object_underscore = obj_name.replace(' ', '_')\n",
    "        object_mask_path = os.path.join(orig_pred_dir_for_example, f'{object_underscore}.png')\n",
    "\n",
    "        if not os.path.exists(object_mask_path):\n",
    "            continue\n",
    "            \n",
    "        bbox = get_bbox_from_mask(object_mask_path)\n",
    "        if bbox is None:\n",
    "            continue\n",
    "\n",
    "        parts_masks = anno['parts']\n",
    "        for part_name, masks in parts_masks.items():\n",
    "            part_mask = combine_masks(masks)\n",
    "            gt_part_masks.append(part_mask)\n",
    "            part_full_name = obj_name + \"_\" + part_name + '_crop'\n",
    "            crop_pred_path = os.path.join(base_pred_dir, filename, f'{part_full_name}.png')\n",
    "            # Transform to full image coordinates\n",
    "            full_pred_mask = transform_crop_mask_to_full_image(crop_pred_path, bbox, original_shape)\n",
    "            if full_pred_mask is None:\n",
    "                skip_image = True\n",
    "                break\n",
    "            pred_part_masks.append(full_pred_mask)\n",
    "        if skip_image:\n",
    "            break\n",
    "    if skip_image:\n",
    "        skip_image = False\n",
    "        continue\n",
    "\n",
    "    if len(pred_part_masks) == 0:\n",
    "        # no part present in this image\n",
    "        print(f'no parts present in {filename}, skipping...')\n",
    "        count_no_obj += 1\n",
    "        continue\n",
    "    \n",
    "    # obj_iou_w_bg, obj_confusion = compute_miou(pred_obj_masks, gt_obj_masks)\n",
    "    # obj_ious.append(obj_iou_w_bg[1]) # only taking the iou of the object class, not the background\n",
    "\n",
    "\n",
    "    part_iou_w_bg, part_confusion = compute_miou(pred_part_masks, gt_part_masks)\n",
    "    part_ious.append(part_iou_w_bg[1]) # only taking the iou of the part class, not the background\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in tqdm(req_filenames):\n",
    "#     img_filepath = os.path.join(pascal_image_dir, filename+'.jpg')\n",
    "#     original_image = cv2.imread(img_filepath)\n",
    "#     original_shape = original_image.shape\n",
    "    \n",
    "#     annot_filename = filename + '.mat'\n",
    "#     classes_to_detect = get_pascalpart_classes(annot_filename, annotations_path)\n",
    "    \n",
    "#     save_dir_for_example = os.path.join(orig_pred_dir, filename)\n",
    "#     crop_save_dir = os.path.join(save_dir, filename + \"_crops\")\n",
    "    \n",
    "#     file_results = {}\n",
    "    \n",
    "#     for object, parts in classes_to_detect.items():\n",
    "#         object_underscore = object.replace(' ', '_')\n",
    "#         object_mask_path = os.path.join(save_dir_for_example, f'{object_underscore}.png')\n",
    "        \n",
    "#         if not os.path.exists(object_mask_path):\n",
    "#             continue\n",
    "            \n",
    "#         bbox = get_bbox_from_mask(object_mask_path)\n",
    "#         if bbox is None:\n",
    "#             continue\n",
    "        \n",
    "#         for part in parts:\n",
    "#             part_underscore = part.replace(' ', '_')\n",
    "            \n",
    "#             # Load crop-based prediction\n",
    "#             crop_pred_path = os.path.join(crop_save_dir, f'{object_underscore}_{part_underscore}_crop.png')\n",
    "#             if not os.path.exists(crop_pred_path):\n",
    "#                 continue\n",
    "            \n",
    "#             # Transform to full image coordinates\n",
    "#             full_pred_mask = transform_crop_mask_to_full_image(crop_pred_path, bbox, original_shape)\n",
    "            \n",
    "#             # Load ground truth (you'll need to implement this based on your GT format)\n",
    "#             gt_mask = load_ground_truth_part_mask(filename, object, part)  # You need to implement this\n",
    "            \n",
    "#             # Calculate IoU\n",
    "#             if gt_mask is not None:\n",
    "#                 iou = calculate_iou(full_pred_mask, gt_mask)\n",
    "#                 file_results[f'{object}_{part}'] = iou\n",
    "    \n",
    "#     results[filename] = file_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
