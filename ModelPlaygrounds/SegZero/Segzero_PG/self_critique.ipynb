{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5723f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ksmehrab/AttentionGrounding/ModelPlaygrounds/SegZero/GitRepoLatest/Seg-Zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95391f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksmehrab/miniconda/envs/segzero/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import argparse\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "import json\n",
    "import pdb\n",
    "\n",
    "import cv2\n",
    "from PIL import Image as PILImage\n",
    "import re\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7feb7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration object, to replace args\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.reasoning_model_path = \"/home/ksmehrab/AttentionGrounding/ModelPlaygrounds/SegZero/GitRepoLatest/Seg-Zero/pretrained_models/VisionReasoner-7B\"\n",
    "        self.segmentation_model_path = \"facebook/sam2-hiera-large\"\n",
    "\n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eced3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_bbox_points_think(output_text, x_factor, y_factor):\n",
    "    json_match = re.search(r'<answer>\\s*(.*?)\\s*</answer>', output_text, re.DOTALL)\n",
    "    json_string = json_match.group(1)\n",
    "    if json_match:\n",
    "        data = json.loads(json_string)\n",
    "        pred_bboxes = [[\n",
    "            int(item['bbox_2d'][0] * x_factor + 0.5),\n",
    "            int(item['bbox_2d'][1] * y_factor + 0.5),\n",
    "            int(item['bbox_2d'][2] * x_factor + 0.5),\n",
    "            int(item['bbox_2d'][3] * y_factor + 0.5)\n",
    "        ] for item in data]\n",
    "        pred_points = [[\n",
    "            int(item['point_2d'][0] * x_factor + 0.5),\n",
    "            int(item['point_2d'][1] * y_factor + 0.5)\n",
    "        ] for item in data]\n",
    "    \n",
    "    think_pattern = r'<think>([^<]+)</think>'\n",
    "    think_match = re.search(think_pattern, output_text)\n",
    "    think_text = \"\"\n",
    "    if think_match:\n",
    "        think_text = think_match.group(1)\n",
    "    \n",
    "    return json_string, pred_bboxes, pred_points, think_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5317ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bbox_points_think(output_text, x_factor, y_factor):\n",
    "    json_match = re.search(r'<answer>\\s*(.*?)\\s*</answer>', output_text, re.DOTALL)\n",
    "    if json_match:\n",
    "        data = json.loads(json_match.group(1))\n",
    "        pred_bboxes = [[\n",
    "            int(item['bbox_2d'][0] * x_factor + 0.5),\n",
    "            int(item['bbox_2d'][1] * y_factor + 0.5),\n",
    "            int(item['bbox_2d'][2] * x_factor + 0.5),\n",
    "            int(item['bbox_2d'][3] * y_factor + 0.5)\n",
    "        ] for item in data]\n",
    "        pred_points = [[\n",
    "            int(item['point_2d'][0] * x_factor + 0.5),\n",
    "            int(item['point_2d'][1] * y_factor + 0.5)\n",
    "        ] for item in data]\n",
    "    \n",
    "    think_pattern = r'<think>([^<]+)</think>'\n",
    "    think_match = re.search(think_pattern, output_text)\n",
    "    think_text = \"\"\n",
    "    if think_match:\n",
    "        think_text = think_match.group(1)\n",
    "    \n",
    "    return pred_bboxes, pred_points, think_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "680c167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bbox(bbox, x_factor, y_factor):\n",
    "    x1 = int(bbox[0] * x_factor + 0.5)\n",
    "    y1 = int(bbox[1] * y_factor + 0.5)\n",
    "    x2 = int(bbox[2] * x_factor + 0.5)\n",
    "    y2 = int(bbox[3] * y_factor + 0.5)\n",
    "\n",
    "    return [x1, y1, x2, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13dfe4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_point(point, x_factor, y_factor):\n",
    "    x = int(point[0] * x_factor + 0.5)\n",
    "    y = int(point[1] * y_factor + 0.5)\n",
    "\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98123dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_json_array_loads(json_string):\n",
    "    \"\"\"\n",
    "    Safely load a JSON string which contains a list of dictionaries. If it fails, return an empty list.\n",
    "\n",
    "    Args:\n",
    "        json_string (str): The JSON string to be parsed.\n",
    "    Returns:\n",
    "        list: The parsed JSON object, or an empty list if parsing fails. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        return json.loads(json_string)\n",
    "    except json.JSONDecodeError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "208f5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_inner_lists_empty(list_of_lists):\n",
    "    \"\"\"\n",
    "    Check if all inner lists in a list of lists are empty.\n",
    "    \n",
    "    Args:\n",
    "        list_of_lists: A list containing other lists\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if all inner lists are empty, False otherwise\n",
    "    \"\"\"\n",
    "    # Check if the outer list itself is empty\n",
    "    if not list_of_lists:\n",
    "        return True\n",
    "    \n",
    "    # Check if all inner lists are empty\n",
    "    return all(len(inner_list) == 0 for inner_list in list_of_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66667a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_clean_json_string(text):\n",
    "#     \"\"\"\n",
    "#     Extract clean JSON string from text that may contain markdown code blocks or other formatting.\n",
    "    \n",
    "#     Args:\n",
    "#         text (str): Input text containing JSON\n",
    "        \n",
    "#     Returns:\n",
    "#         str: Clean JSON string, or empty string if no valid JSON found\n",
    "#     \"\"\"\n",
    "#     import re\n",
    "#     import json\n",
    "    \n",
    "#     # First, try to find JSON within markdown code blocks\n",
    "#     markdown_pattern = r'```(?:json)?\\s*(.*?)\\s*```'\n",
    "#     markdown_match = re.search(markdown_pattern, text, re.DOTALL)\n",
    "    \n",
    "#     if markdown_match:\n",
    "#         json_candidate = markdown_match.group(1).strip()\n",
    "#     else:\n",
    "#         # If no markdown blocks, look for JSON-like structures\n",
    "#         # Look for content between [ ] or { }\n",
    "#         bracket_pattern = r'(\\[.*?\\]|\\{.*?\\})'\n",
    "#         bracket_match = re.search(bracket_pattern, text, re.DOTALL)\n",
    "#         if bracket_match:\n",
    "#             json_candidate = bracket_match.group(1).strip()\n",
    "#         else:\n",
    "#             json_candidate = text.strip()\n",
    "    \n",
    "#     # Clean up common formatting issues\n",
    "#     json_candidate = json_candidate.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "#     json_candidate = re.sub(r'\\s+', ' ', json_candidate)  # Normalize whitespace\n",
    "\n",
    "#     print(json_candidate)\n",
    "    \n",
    "#     # Validate that it's proper JSON\n",
    "#     try:\n",
    "#         json.loads(json_candidate)\n",
    "#         return json_candidate\n",
    "#     except json.JSONDecodeError:\n",
    "#         print(\"Invalid JSON detected.\")\n",
    "#         return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5ab67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_json_string(text):\n",
    "    \"\"\"\n",
    "    Extract clean JSON string from text that may contain markdown code blocks or other formatting.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text containing JSON\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean JSON string, or empty string if no valid JSON found\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, try to find JSON within markdown code blocks\n",
    "    markdown_pattern = r'```(?:json)?\\s*(.*?)\\s*```'\n",
    "    markdown_match = re.search(markdown_pattern, text, re.DOTALL)\n",
    "    \n",
    "    if markdown_match:\n",
    "        json_candidate = markdown_match.group(1).strip()\n",
    "    else:\n",
    "        # Function to find matching bracket\n",
    "        def find_matching_bracket(text, start_pos, open_char, close_char):\n",
    "            count = 1\n",
    "            pos = start_pos + 1\n",
    "            while pos < len(text) and count > 0:\n",
    "                if text[pos] == open_char:\n",
    "                    count += 1\n",
    "                elif text[pos] == close_char:\n",
    "                    count -= 1\n",
    "                pos += 1\n",
    "            return pos if count == 0 else -1\n",
    "        \n",
    "        # Look for JSON array or object\n",
    "        json_candidate = \"\"\n",
    "        \n",
    "        # First try to find array starting with [\n",
    "        array_start = text.find('[')\n",
    "        if array_start != -1:\n",
    "            array_end = find_matching_bracket(text, array_start, '[', ']')\n",
    "            if array_end != -1:\n",
    "                json_candidate = text[array_start:array_end]\n",
    "        \n",
    "        # If no valid array found, try to find object starting with {\n",
    "        if not json_candidate:\n",
    "            obj_start = text.find('{')\n",
    "            if obj_start != -1:\n",
    "                obj_end = find_matching_bracket(text, obj_start, '{', '}')\n",
    "                if obj_end != -1:\n",
    "                    json_candidate = text[obj_start:obj_end]\n",
    "        \n",
    "        # If still no candidate, fallback to original approach\n",
    "        if not json_candidate:\n",
    "            json_candidate = text.strip()\n",
    "    \n",
    "    # Clean up common formatting issues\n",
    "    json_candidate = json_candidate.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "    json_candidate = re.sub(r'\\s+', ' ', json_candidate)  # Normalize whitespace\n",
    "\n",
    "    # print(json_candidate)\n",
    "    \n",
    "    # Validate that it's proper JSON\n",
    "    try:\n",
    "        json.loads(json_candidate)\n",
    "        return json_candidate\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Invalid JSON detected.\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95d0e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(mask1, mask2):\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return intersection, union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27cc3c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bbox_iou(bbox1, bbox2):\n",
    "    \"\"\"\n",
    "    Compute IoU (Intersection over Union) between two bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        bbox1: List or array [x1, y1, x2, y2] representing first bbox\n",
    "        bbox2: List or array [x1, y1, x2, y2] representing second bbox\n",
    "        \n",
    "    Returns:\n",
    "        float: IoU value between 0 and 1\n",
    "    \"\"\"\n",
    "\n",
    "    x1_1, y1_1, x2_1, y2_1 = bbox1\n",
    "    x1_2, y1_2, x2_2, y2_2 = bbox2\n",
    "    \n",
    "    # Calculate intersection coordinates\n",
    "    x1_intersect = max(x1_1, x1_2)\n",
    "    y1_intersect = max(y1_1, y1_2)\n",
    "    x2_intersect = min(x2_1, x2_2)\n",
    "    y2_intersect = min(y2_1, y2_2)\n",
    "    \n",
    "    # Check if there's no intersection\n",
    "    if x1_intersect >= x2_intersect or y1_intersect >= y2_intersect:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate intersection area\n",
    "    intersection_area = (x2_intersect - x1_intersect) * (y2_intersect - y1_intersect)\n",
    "    \n",
    "    # Calculate areas of both bboxes\n",
    "    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "    \n",
    "    # Calculate union area\n",
    "    union_area = area1 + area2 - intersection_area\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if union_area == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = intersection_area / union_area\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7b42086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_masks(masks):\n",
    "    combined_mask = masks[0]\n",
    "    for i in range(1, len(masks)):\n",
    "        combined_mask = combined_mask | masks[i]\n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a706176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "#We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "reasoning_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    args.reasoning_model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdffd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2267f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_model = SAM2ImagePredictor.from_pretrained(args.segmentation_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec512e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(args.reasoning_model_path, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "817207f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_TEMPLATE = \\\n",
    "    \"Please find \\\"{Question}\\\" with bboxs and points.\" \\\n",
    "    \"Compare the difference between object(s) or parts and find the most closely matched object(s) or parts.\" \\\n",
    "    \"Output the thinking process in <think> </think> and final answer in <answer> </answer> tags.\" \\\n",
    "    \"Output the bbox(es) and point(s) inside the interested object(s) or parts in JSON format.\" \\\n",
    "    \"i.e., <think> thinking process here </think>\" \\\n",
    "    \"<answer>{Answer}</answer>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2a5373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The box caption template will be used to caption the first predicted region/s \n",
    "BOX_CAPTION_TEMPLATE = \"Please label the regions inside the provided bboxes. Be concise. Output the same JSON as provided, adding a \\\"label\\\" field per region. Regions to label: \\n <JSON_BBOX_INFO>\"\n",
    "\n",
    "# BOX_CAPTION_TEMPLATE = \"Please label the regions inside the provided bboxes. Be concise. Output the same JSON as provided, adding a \\\"label\\\" field per region. Do not generate any new bboxes. Only label the regions in the provided bboxes. Regions to label: \\n <JSON_BBOX_INFO>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "900c0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self critique template, similar to QUESTION_TEMPLATE but adds the previous answer for self-evaluation \n",
    "# TODO: Should we change {Answer} to the previous answer? \n",
    "# TODO: Should we extract the caption from the previous answer and provide it as a separate <CAPTION_INFO>?\n",
    "\n",
    "# SELF_CRITIQUE_TEMPLATE = \\\n",
    "#     \"The following bbox(es) and captions are provided for your reference as a previous answer: <JSON_BBOX_INFO>.\" \\\n",
    "#     \"Please find \\\"{Question}\\\" with bboxs and points.\" \\\n",
    "#     \"Based on the reference bbox(es) and point(s), decide if the previous answer was correct or not.\" \\\n",
    "#     \"Think if the predicted bbox(es) need to be bigger, smaller or the same to accurately find \\\"{Question}\\\"\" \\\n",
    "#     \"Output the thinking process in <think> </think> and final answer in <answer> </answer> tags.\" \\\n",
    "#     \"Output the bbox(es) and point(s) inside the interested object(s) or parts in JSON format.\" \\\n",
    "#     \"i.e., <think> thinking process here </think>\" \\\n",
    "#     \"<answer>{Answer}</answer>\"\n",
    "\n",
    "SELF_CRITIQUE_TEMPLATE = \\\n",
    "    \"Please find \\\"{Question}\\\" with bbox(es) and points. \" \\\n",
    "    \"The following bbox(es) were your previous answer to find \\\"{Question}\\\": <JSON_BBOX_INFO>. \" \\\n",
    "    \"Here are the captions of you what you actually found in the previous bbox(es): <JSON_CAPTION_INFO> \" \\\n",
    "    \"Compare what you need to find --  \\\"{Question}\\\" with the captioned bboxes to decide if the previous answer was correct or not. \" \\\n",
    "    \"The bbox(es) should only enclose \\\"{Question}\\\" for it to be correct.\" \\\n",
    "    \"For example, if your task is to find \\\"person's head\\\", but the caption of the bbox(es) say that it is the entire person, then the previous answer is incorrect. \" \\\n",
    "    \"If the previous bbox(es) was incorrect, think if the correct bbox(es) need to be bigger, smaller or the same to accurately find \\\"{Question}\\\". \" \\\n",
    "    \"Output the thinking process in <think> </think> and final answer in <answer> </answer> tags. \" \\\n",
    "    \"Output the bbox(es) and point(s) inside the interested object(s) or parts in JSON format. \" \\\n",
    "    \"i.e., <think> thinking process here </think> \" \\\n",
    "    \"<answer>{Answer}</answer>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d67ac6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outputs_for_self_critique_template(\n",
    "    filename, \n",
    "    seg_list,\n",
    "    json_bboxes_strings, \n",
    "    image, \n",
    "    resize_size=840\n",
    "):\n",
    "    \"\"\" \n",
    "    Pass 3\n",
    "    \"\"\" \n",
    "    assert len(seg_list) == len(json_bboxes_strings)\n",
    "    original_width, original_height = image.size\n",
    "    messages = []\n",
    "    for prev_output, seg_name in zip(json_bboxes_strings, seg_list):\n",
    "        # separate the previous output into bbox and caption\n",
    "        prev_output_json_string = extract_clean_json_string(prev_output)\n",
    "        prev_output_json = safe_json_array_loads(prev_output_json_string)\n",
    "        prev_bboxes = [item['bbox_2d'] for item in prev_output_json if 'bbox_2d' in item]\n",
    "        prev_captions = [item['label'] for item in prev_output_json if 'label' in item]\n",
    "\n",
    "        # convert the separate prev_bboxes and prev_captions into a JSON string\n",
    "        prev_bboxes_json_string = json.dumps([{'bbox_2d': bbox} for bbox in prev_bboxes])\n",
    "        prev_captions_json_string = json.dumps([{'caption': caption} for caption in prev_captions])\n",
    "\n",
    "        formatted_template = SELF_CRITIQUE_TEMPLATE.format(\n",
    "            Question=seg_name.lower().strip(\".\"),\n",
    "            Answer=\"[{\\\"bbox_2d\\\": [10,100,200,210], \\\"point_2d\\\": [30,110]}, {\\\"bbox_2d\\\": [225,296,706,786], \\\"point_2d\\\": [302,410]}]\"\n",
    "        )\n",
    "        final_text = formatted_template.replace(\n",
    "            \"<JSON_BBOX_INFO>\", prev_bboxes_json_string).replace(\n",
    "                \"<JSON_CAPTION_INFO>\", prev_captions_json_string)\n",
    "    \n",
    "        message = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\", \n",
    "                \"image\": image.resize((resize_size, resize_size), PILImage.BILINEAR)\n",
    "            },\n",
    "            {   \n",
    "                \"type\": \"text\",\n",
    "                \"text\": final_text\n",
    "            }\n",
    "        ]\n",
    "        }]\n",
    "        messages.append(message)\n",
    "    \n",
    "    # print(messages)\n",
    "    \n",
    "    # Preparation for inference\n",
    "    text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages]\n",
    "\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "\n",
    "    with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        generated_ids = reasoning_model.generate(**inputs, use_cache=True, max_new_tokens=1024, do_sample=False)\n",
    "\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        batch_output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        # print(batch_output_text)\n",
    "        batch_json_strings = []\n",
    "        batch_bboxes = []\n",
    "        batch_points = []\n",
    "        batch_thinks = []\n",
    "\n",
    "        for id_idx in range(len(batch_output_text)):\n",
    "            try:\n",
    "                json_string, bboxes, points, think = extract_json_bbox_points_think(\n",
    "                                        batch_output_text[id_idx], \n",
    "                                        original_width/resize_size, \n",
    "                                        original_height/resize_size\n",
    "                                    )\n",
    "            except Exception as e:\n",
    "                # assign empty values if there is an exception\n",
    "                print(\"Reasoning error: \", e, \"Text: \", batch_output_text[id_idx], \"Image: \", filename, \"Seg: \", seg_list[id_idx])\n",
    "                think = \"\"\n",
    "                bboxes = []\n",
    "                points = []\n",
    "                json_string = \"\"\n",
    "\n",
    "            batch_json_strings.append(json_string)\n",
    "            batch_bboxes.append(bboxes)\n",
    "            batch_points.append(points)\n",
    "            batch_thinks.append(think)\n",
    "\n",
    "    # clean GPU memory\n",
    "    del inputs, generated_ids, generated_ids_trimmed\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return batch_json_strings, batch_bboxes, batch_points, batch_thinks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0a1d6b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outputs_for_caption_template(\n",
    "    filename, \n",
    "    json_bboxes_strings, \n",
    "    image, \n",
    "    resize_size=840\n",
    "):\n",
    "    \"\"\" \n",
    "    Pass 2\n",
    "    \"\"\"\n",
    "    # TODO: handle situation where an element in the json_bboxes_strings is empty. \n",
    "    # This can happen if the first pass detecting bbox does not detect anything.\n",
    "    # Or if the first pass runs into an exception \n",
    "\n",
    "    original_width, original_height = image.size\n",
    "    messages = []\n",
    "    for args_text in json_bboxes_strings:\n",
    "        message = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\", \n",
    "                \"image\": image.resize((resize_size, resize_size), PILImage.BILINEAR)\n",
    "            },\n",
    "            {   \n",
    "                \"type\": \"text\",\n",
    "                \"text\": BOX_CAPTION_TEMPLATE.replace(\"<JSON_BBOX_INFO>\", args_text)\n",
    "            }\n",
    "        ]\n",
    "        }]\n",
    "        messages.append(message)\n",
    "    \n",
    "    # print(messages)\n",
    "    \n",
    "    # Preparation for inference\n",
    "    text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages]\n",
    "\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "\n",
    "    with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        generated_ids = reasoning_model.generate(**inputs, use_cache=True, max_new_tokens=1024, do_sample=False)\n",
    "\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        batch_output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "    output_texts_cleaned = []\n",
    "    for output_text in batch_output_text:\n",
    "        output_texts_cleaned.append(extract_clean_json_string(output_text))\n",
    "        \n",
    "    # clean GPU memory\n",
    "    del inputs, generated_ids, generated_ids_trimmed\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return output_texts_cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ab9e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outputs_for_question_template(\n",
    "    filename,\n",
    "    seg_list,\n",
    "    image,\n",
    "    resize_size=840,\n",
    "):\n",
    "    \"\"\" \n",
    "    Pass 1 \n",
    "    \"\"\"\n",
    "    original_width, original_height = image.size\n",
    "    messages = []\n",
    "    for args_text in seg_list:\n",
    "        message = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\", \n",
    "                \"image\": image.resize((resize_size, resize_size), PILImage.BILINEAR)\n",
    "            },\n",
    "            {   \n",
    "                \"type\": \"text\",\n",
    "                \"text\": QUESTION_TEMPLATE.format(\n",
    "                    Question=args_text.lower().strip(\".\"),\n",
    "                    Answer=\"[{\\\"bbox_2d\\\": [10,100,200,210], \\\"point_2d\\\": [30,110]}, {\\\"bbox_2d\\\": [225,296,706,786], \\\"point_2d\\\": [302,410]}]\"\n",
    "                )    \n",
    "            }\n",
    "        ]\n",
    "        }]\n",
    "        messages.append(message)\n",
    "    \n",
    "    # Preparation for inference\n",
    "    text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages]\n",
    "\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "\n",
    "    with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        generated_ids = reasoning_model.generate(**inputs, use_cache=True, max_new_tokens=1024, do_sample=False)\n",
    "\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        batch_output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        # print(batch_output_text)\n",
    "        batch_json_strings = []\n",
    "        batch_bboxes = []\n",
    "        batch_points = []\n",
    "        batch_thinks = []\n",
    "\n",
    "        for id_idx in range(len(batch_output_text)):\n",
    "            try:\n",
    "                json_string, bboxes, points, think = extract_json_bbox_points_think(\n",
    "                                        batch_output_text[id_idx], \n",
    "                                        original_width/resize_size, \n",
    "                                        original_height/resize_size\n",
    "                                    )\n",
    "            except Exception as e:\n",
    "                # assign empty values if there is an exception\n",
    "                print(\"Reasoning error: \", e, \"Text: \", batch_output_text[id_idx], \"Image: \", filename, \"Seg: \", seg_list[id_idx])\n",
    "                think = \"\"\n",
    "                bboxes = []\n",
    "                points = []\n",
    "                json_string = \"\"\n",
    "\n",
    "            batch_json_strings.append(json_string)\n",
    "            batch_bboxes.append(bboxes)\n",
    "            batch_points.append(points)\n",
    "            batch_thinks.append(think)\n",
    "\n",
    "    # clean GPU memory\n",
    "    del inputs, generated_ids, generated_ids_trimmed\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return batch_json_strings, batch_bboxes, batch_points, batch_thinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5479fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import json\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "def visualize_bbox_comparison(\n",
    "        image, \n",
    "        pass1_json_strings, \n",
    "        pass2_json_strings_cleaned, \n",
    "        pass3_input_json_strings,\n",
    "        pass3_output_bboxes, \n",
    "        pass3_predicted_masks, \n",
    "        gt_masks,\n",
    "        pass3_output_thinks,\n",
    "        seg_list, \n",
    "        filename,\n",
    "        x_factor, \n",
    "        y_factor\n",
    "):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization comparing Pass 1 and Pass 2 results\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image object\n",
    "        pass1_json_strings: List of JSON strings from Pass 1 (QUESTION_TEMPLATE)\n",
    "        pass2_json_strings_cleaned: List of cleaned JSON strings from Pass 2 (BOX_CAPTION_TEMPLATE)\n",
    "        pass3_input_json_strings: List of JSON strings used as input for Pass 3 (SELF_CRITIQUE_TEMPLATE)\n",
    "        seg_list: List of object/part names\n",
    "        filename: Image filename for saving plots\n",
    "    \"\"\"\n",
    "\n",
    "    for i, (pass1_json_str, pass2_json_str, pass3_input_json_str, pass3_output_bbox, pass3_predicted_mask, gt_mask, pass3_output_think, seg_name) in enumerate(zip(pass1_json_strings, pass2_json_strings_cleaned, pass3_input_json_strings, pass3_output_bboxes, pass3_predicted_masks, gt_masks, pass3_output_thinks, seg_list)):\n",
    "        # Skip if pass1_json_str is empty (no detection in Pass 1)\n",
    "        if not pass1_json_str:\n",
    "            print(f\"Skipping {seg_name} - no detection in Pass 1\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Parse Pass 1 results (initial bboxes)\n",
    "            pass1_data = json.loads(pass1_json_str)\n",
    "            pass1_bboxes = [normalize_bbox(item['bbox_2d'], x_factor, y_factor) for item in pass1_data]\n",
    "            \n",
    "            # Parse Pass 2 results (captioned bboxes)\n",
    "            if pass2_json_str:\n",
    "                pass2_data = json.loads(pass2_json_str)\n",
    "                pass2_bboxes = [normalize_bbox(item['bbox_2d'], x_factor, y_factor) for item in pass2_data]\n",
    "                pass2_labels = [item['label'] for item in pass2_data]\n",
    "            else:\n",
    "                pass2_bboxes = []\n",
    "                pass2_labels = []\n",
    "\n",
    "            # Parse Pass 3 input results (captioned bboxes to use for self critique)\n",
    "            if pass3_input_json_str:\n",
    "                pass3_input_data = json.loads(pass3_input_json_str)\n",
    "                pass3_input_bboxes = [normalize_bbox(item['bbox_2d'], x_factor, y_factor) for item in pass3_input_data]\n",
    "                pass3_input_labels = [item['label'] for item in pass3_input_data]\n",
    "            else:\n",
    "                pass3_input_bboxes = []\n",
    "                pass3__input_labels = []\n",
    "            \n",
    "            # Create visualization\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 6))\n",
    "            fig.suptitle(f'Bbox Analysis for: {seg_name}', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # Plot 1: Pass 1 bboxes (initial detection)\n",
    "            ax1 = axes[0, 0]\n",
    "            ax1.imshow(image)\n",
    "            ax1.set_title(f'Pass 1: Initial Detection\\n({len(pass1_bboxes)} bboxes)')\n",
    "            ax1.axis('off')\n",
    "            \n",
    "            for j, bbox in enumerate(pass1_bboxes):\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                       linewidth=2, edgecolor='red', facecolor='none')\n",
    "                ax1.add_patch(rect)\n",
    "                ax1.text(x1, y1-5, f'bbox {j+1}', fontsize=10, color='red', \n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
    "            \n",
    "            # Plot 2: Pass 2 bboxes with labels (same image, all bboxes)\n",
    "            ax2 = axes[0, 1]\n",
    "            ax2.imshow(image)\n",
    "            ax2.set_title(f'Pass 2: Captioned Bboxes\\n({len(pass2_bboxes)} bboxes)')\n",
    "            ax2.axis('off')\n",
    "            \n",
    "            colors = ['blue', 'green', 'purple', 'orange', 'cyan']\n",
    "            for j, (bbox, label) in enumerate(zip(pass2_bboxes, pass2_labels)):\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                color = colors[j % len(colors)]\n",
    "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                       linewidth=2, edgecolor=color, facecolor='none')\n",
    "                ax2.add_patch(rect)\n",
    "                ax2.text(x1, y1-5, f'{label}', fontsize=10, color=color, \n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
    "            \n",
    "            # Plot 3: Pass 3 input bboxes with labels (the output of pass 2 is filtered and used as input for pass 3)\n",
    "            ax3 = axes[0, 2]\n",
    "            ax3.imshow(image)\n",
    "\n",
    "            ax3.set_title(f'Pass 3: Self-Critique Input Bboxes\\n({len(pass3_input_bboxes)} bboxes)')\n",
    "            ax3.axis('off') \n",
    "            for j, (bbox, label) in enumerate(zip(pass3_input_bboxes, pass3_input_labels)):\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                color = colors[j % len(colors)]\n",
    "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                       linewidth=2, edgecolor=color, facecolor='none', linestyle='--')\n",
    "                ax3.add_patch(rect)\n",
    "                ax3.text(x1, y1-5, f'{label}', fontsize=10, color=color, \n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
    "            \n",
    "            ax4 = axes[1, 0]\n",
    "            ax4.imshow(image)\n",
    "            ax4.set_title(f'Pass 3: Final BBox Prediction for \\\"{seg_name}\\\"')\n",
    "            ax4.axis('off')\n",
    "            for bbox in pass3_output_bbox:\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                color = colors[j % len(colors)]\n",
    "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                       linewidth=2, edgecolor=color, facecolor='none', linestyle='--')\n",
    "                ax4.add_patch(rect)\n",
    "            \n",
    "            ax5 = axes[1, 1]\n",
    "            ax5.imshow(image)\n",
    "\n",
    "            predicted_overlay = np.zeros((pass3_predicted_mask.shape[0], pass3_predicted_mask.shape[1], 4))\n",
    "            predicted_overlay[pass3_predicted_mask, 0] = 1.0  # Red color for prediction\n",
    "            predicted_overlay[pass3_predicted_mask, 3] = 0.5  # Alpha transparency\n",
    "            ax5.imshow(predicted_overlay)\n",
    "            ax5.set_title(f'Predicted: {seg_name}')\n",
    "            ax5.axis('off')\n",
    "\n",
    "            ax6 = axes[1, 2]\n",
    "            ax6.imshow(image)\n",
    "            gt_mask = np.array(gt_mask)\n",
    "            gt_overlay = np.zeros((gt_mask.shape[0], gt_mask.shape[1], 4))\n",
    "            gt_overlay[gt_mask.astype(bool), 1] = 1.0  # Green color for ground truth\n",
    "            gt_overlay[gt_mask.astype(bool), 3] = 0.5  # Alpha transparency\n",
    "            ax6.imshow(gt_overlay)\n",
    "            ax6.set_title(f'Ground Truth: {seg_name}')\n",
    "            ax6.axis('off')\n",
    "\n",
    "            # Add text box for thinking process\n",
    "            fig.text(0.02, 0.02, pass3_output_think, fontsize=10, \n",
    "                     bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightgray', alpha=0.8),\n",
    "                     verticalalignment='bottom')\n",
    "\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            # plt.savefig(f'bbox_analysis_{filename}_{i}_{seg_name.replace(\"/\", \"_\").replace(\" \", \"_\")}.png', \n",
    "            #            dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "\n",
    "            # ax3.set_title('Comparison: Pass 1 (red) vs Pass 2 (colored)')\n",
    "            # ax3.axis('off')\n",
    "            \n",
    "            # # Draw Pass 1 bboxes in red\n",
    "            # for j, bbox in enumerate(pass1_bboxes):\n",
    "            #     x1, y1, x2, y2 = bbox\n",
    "            #     rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "            #                            linewidth=2, edgecolor='red', facecolor='none', \n",
    "            #                            linestyle='--', alpha=0.7)\n",
    "            #     ax3.add_patch(rect)\n",
    "            \n",
    "            # # Draw Pass 2 bboxes in colors\n",
    "            # for j, (bbox, label) in enumerate(zip(pass2_bboxes, pass2_labels)):\n",
    "            #     x1, y1, x2, y2 = bbox\n",
    "            #     color = colors[j % len(colors)]\n",
    "            #     rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "            #                            linewidth=2, edgecolor=color, facecolor='none')\n",
    "            #     ax3.add_patch(rect)\n",
    "\n",
    "\n",
    "            \n",
    "            # Additional individual bbox visualizations for Pass 2\n",
    "            # if len(pass2_bboxes) > 1:\n",
    "            #     fig2, axes2 = plt.subplots(1, len(pass2_bboxes), figsize=(6*len(pass2_bboxes), 6))\n",
    "            #     if len(pass2_bboxes) == 1:\n",
    "            #         axes2 = [axes2]\n",
    "                    \n",
    "            #     fig2.suptitle(f'Individual Pass 2 Bboxes for: {seg_name}', fontsize=16, fontweight='bold')\n",
    "                \n",
    "            #     for j, (bbox, label) in enumerate(zip(pass2_bboxes, pass2_labels)):\n",
    "            #         ax = axes2[j]\n",
    "            #         ax.imshow(image)\n",
    "            #         ax.set_title(f'Bbox {j+1}: {label}')\n",
    "            #         ax.axis('off')\n",
    "                    \n",
    "            #         x1, y1, x2, y2 = bbox\n",
    "            #         color = colors[j % len(colors)]\n",
    "            #         rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "            #                                linewidth=3, edgecolor=color, facecolor='none')\n",
    "            #         ax.add_patch(rect)\n",
    "                \n",
    "            #     plt.tight_layout()\n",
    "            #     # plt.savefig(f'individual_bboxes_{filename}_{i}_{seg_name.replace(\"/\", \"_\").replace(\" \", \"_\")}.png', \n",
    "            #     #            dpi=150, bbox_inches='tight')\n",
    "            #     plt.show()\n",
    "            \n",
    "            # Print comparison information\n",
    "            # print(f\"\\n=== Analysis for {seg_name} ===\")\n",
    "            # print(f\"Pass 1 detected {len(pass1_bboxes)} bboxes\")\n",
    "            # print(f\"Pass 2 captioned {len(pass2_bboxes)} bboxes\")\n",
    "            \n",
    "            # if pass2_labels:\n",
    "            #     print(\"Pass 2 Labels:\")\n",
    "            #     for j, label in enumerate(pass2_labels):\n",
    "            #         print(f\"  Bbox {j+1}: {label}\")\n",
    "                    \n",
    "            #     # Check if labels match expected seg_name\n",
    "            #     expected_lower = seg_name.lower()\n",
    "            #     matching_labels = [label for label in pass2_labels if expected_lower in label.lower() or any(word in label.lower() for word in expected_lower.split())]\n",
    "            #     print(f\"Labels matching '{seg_name}': {len(matching_labels)}/{len(pass2_labels)}\")\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON for {seg_name}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {seg_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5095a4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [04:06<09:18, 79.73s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning error:  'point_2d' Text:  <think> The previous answer correctly identified the headlight of the train. The captions \"headlight\" match the objects in the provided bboxes, which are indeed the headlights of the train. There is no need to adjust the bboxes as they are already correctly placed.</think>\n",
      "<answer>[{\"bbox_2d\": [540, 341, 568, 366]}, {\"bbox_2d\": [647, 348, 666, 370]}]</answer> Image:  2009_003105 Seg:  train's headlight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [10:59<00:00, 65.91s/it]\n"
     ]
    }
   ],
   "source": [
    "## Code for getting pascalpart images and object/object parts \n",
    "sys.path.append('/home/ksmehrab/AttentionGrounding/Baselines/Models/GLAMM')\n",
    "sys.path.append('/home/ksmehrab/AttentionGrounding/Baselines/Datasets')\n",
    "sys.path.append('/home/ksmehrab/AttentionGrounding/Baselines')\n",
    "\n",
    "from pascalpart import get_pascalpart_classes, get_pascalpart_masks\n",
    "from utils import read_txt_file, save_to_json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "pascal_image_dir = \"/data/Pascal_VOC_2012/VOCdevkit/VOC2012/JPEGImages\" # 17125 images\n",
    "annotations_path= \"/data/PartSegmentationDatasets/PascalPart/Annotations_Part\"\n",
    "val_filepath = \"/data/PartSegmentationDatasets/PascalPart/val.txt\" # 925 images. File contains just the file prefix. Add .jpg extension for images, and .mat extension for annotations\n",
    "val_filenames = read_txt_file(val_filepath)\n",
    "\n",
    "all_object_outputs = []\n",
    "all_parts_outputs = []\n",
    "for filename in tqdm(val_filenames[:10]):\n",
    "    img_filepath = os.path.join(pascal_image_dir, filename+'.jpg')\n",
    "\n",
    "    image = PILImage.open(img_filepath)\n",
    "    image = image.convert(\"RGB\")\n",
    "    original_width, original_height = image.size\n",
    "    resize_size = 840\n",
    "    x_factor, y_factor = original_width/resize_size, original_height/resize_size\n",
    "\n",
    "    annot_filename = filename + '.mat'\n",
    "    anno_dict = get_pascalpart_masks(annot_filename, annotations_path, images_path=pascal_image_dir)\n",
    "    \n",
    "    # classes_to_detect = get_pascalpart_classes(annot_filename, annotations_path) # This is a dictionary in this format {object: [list of parts]}\n",
    "    # print(classes_to_detect)\n",
    "    for obj_name, anno in anno_dict.items():\n",
    "        obj_masks = anno['object_maps']\n",
    "        obj_mask = combine_masks(obj_masks)\n",
    "\n",
    "        gt_mask_list = [obj_mask]\n",
    "        seg_list = [obj_name]\n",
    "\n",
    "        parts_masks = anno['parts']\n",
    "        # print(parts_masks.keys())\n",
    "        for part_name, masks in parts_masks.items():\n",
    "            part_mask = combine_masks(masks)\n",
    "            gt_mask_list.append(part_mask)\n",
    "            part_full_name = obj_name + \"'s \" + part_name\n",
    "            seg_list.append(part_full_name)\n",
    "\n",
    "        # LLM inference needs to happen 3 times, once for the QUESTION_TEMPLATE, once for BOX_CAPTION_TEMPLATE, and once for SELF_CRITIQUE_TEMPLATE\n",
    "\n",
    "        # break the following down into a method with required inputs \n",
    "        # we can create three different methods for each template if needed\n",
    "        # each method will return its own output, which we can pass into the next method for the next template\n",
    "        # This way, we dont need to loop over the templates, just call the methods in sequence\n",
    "        # Begin creating prompt and running inference using the LLM \n",
    "\n",
    "        # A single batch is a single object and its parts. We have all these objects and parts in seg_list\n",
    "        \n",
    "        # Pass 1: QUESTION_TEMPLATE. This will give us the initial bounding boxes and points for the object and its parts\n",
    "        # json_strings are list of strings. Each string corresponds to an object/part name in seg_list that we passed in. \n",
    "        # Each string is of the format: list of dicts. We have multiple dicts in the list if the object/part is present multiple times in the image\n",
    "        # Each dict has keys \"bbox_2d\" and \"point_2d\" \n",
    "        pass1_json_strings, _, _, thinks = generate_outputs_for_question_template(\n",
    "            filename,\n",
    "            seg_list,\n",
    "            image,\n",
    "            resize_size=resize_size\n",
    "        )\n",
    "\n",
    "        # Pass 2: BOX_CAPTION_TEMPLATE. This will give us the caption for the first predicted region\n",
    "        pass2_json_strings_cleaned = generate_outputs_for_caption_template(\n",
    "            filename,\n",
    "            pass1_json_strings,\n",
    "            image,\n",
    "            resize_size=resize_size\n",
    "        )\n",
    "\n",
    "        # print(pass2_json_strings_cleaned)\n",
    "\n",
    "        # We should have the same number of outputs at this stage as the number of objects that we passed in (seg_list)\n",
    "\n",
    "        assert len(pass1_json_strings) == len(seg_list)\n",
    "        assert len(pass2_json_strings_cleaned) == len(seg_list)\n",
    "\n",
    "        # pass2 can generate bboxes that were never present in pass1. \n",
    "        # This requires a filtering step to remove those bboxes from pass2 that were not in pass1.\n",
    "        # Also, add the bboxes from pass1 that were not in pass2 to pass2 with a label of \"unknown\" or \"unlabeled\"\n",
    "\n",
    "        # if the json strings are not valid jsons, we replace them with an empty list. \n",
    "        # This avoids running into JSONDecodeErrors while the code loops\n",
    "        pass1_jsons = [safe_json_array_loads(js) for js in pass1_json_strings]\n",
    "        pass2_jsons = [safe_json_array_loads(js) for js in pass2_json_strings_cleaned]\n",
    "\n",
    "        # Filter pass2_jsons to only include bboxes that were in pass1_jsons\n",
    "        filtered_pass2_jsons = []\n",
    "        for p1, p2 in zip(pass1_jsons, pass2_jsons):\n",
    "            # Recall: p1 and p2 are lists of dicts with keys \"bbox_2d\" and \"point_2d\"\n",
    "            # Sometimes, p1 and p2 can have lists that are empty due to json parsing errors\n",
    "            if all_inner_lists_empty(p1):\n",
    "                # if all p1_bboxes is empty, all of p2 boxes should also be just empty. \n",
    "                filtered_pass2_jsons.append(p1)\n",
    "                # We do not need to process further for this object/part\n",
    "                continue\n",
    "\n",
    "            p1_bboxes = [item['bbox_2d'] for item in p1]\n",
    "\n",
    "            filtered_p2 = []\n",
    "            for item in p2:\n",
    "                item_bbox = item['bbox_2d']\n",
    "                if item_bbox in p1_bboxes:\n",
    "                    filtered_p2.append(item)\n",
    "                    # remove from p1_bboxes to keep track of which bboxes have been matched\n",
    "                    p1_bboxes.remove(item_bbox)\n",
    "                else:\n",
    "                    # if the bbox is not in p1_bboxes, we match it to the closest bbox in p1_bboxes based on bbox IoU. We only consider it a match if bbox IoU > 0.9\n",
    "                    item_bbox_np = np.array(item_bbox)\n",
    "                    best_iou = 0\n",
    "                    best_match = None\n",
    "                    \n",
    "                    for p1_bbox in p1_bboxes:\n",
    "                        p1_bbox_np = np.array(p1_bbox)\n",
    "                        iou = compute_bbox_iou(item_bbox, p1_bbox)\n",
    "                        if iou > best_iou:\n",
    "                            best_iou = iou\n",
    "                            best_match = p1_bbox\n",
    "\n",
    "                    # consider it a match if IoU > 0.9\n",
    "                    if best_iou > 0.9:\n",
    "                        # Add the item with the matched bbox\n",
    "                        item_copy = item.copy()\n",
    "                        item_copy['bbox_2d'] = best_match\n",
    "                        filtered_p2.append(item_copy)\n",
    "                        p1_bboxes.remove(best_match)\n",
    "\n",
    "                        \n",
    "            # Add unmatched bboxes from p1 as \"unlabeled\"\n",
    "            for unmatched_bbox in p1_bboxes:\n",
    "                filtered_p2.append({\n",
    "                    \"bbox_2d\": unmatched_bbox,\n",
    "                    \"label\": \"unlabeled\"\n",
    "                })\n",
    "\n",
    "            filtered_pass2_jsons.append(filtered_p2)\n",
    "        \n",
    "        # Convert back to json strings\n",
    "        # For the objects/parts where pass1 had no bboxes, we will have a list of empty lists in filtered_pass2_jsons. \n",
    "        ## These are also converted to corresponding strings, and passed to pass3 \n",
    "        pass2_json_strings_filtered = [json.dumps(js) for js in filtered_pass2_jsons]\n",
    "\n",
    "\n",
    "        # Pass 3: SELF_CRITIQUE_TEMPLATE. This will give us the refined bounding boxes and points for the object and its parts\n",
    "\n",
    "        \n",
    "        pass3_json_strings, pass3_bboxes, pass3_points, pass3_thinks = generate_outputs_for_self_critique_template(\n",
    "            filename, \n",
    "            seg_list,\n",
    "            pass2_json_strings_filtered, \n",
    "            image, \n",
    "            resize_size=840\n",
    "        )\n",
    "\n",
    "        pass3_predicted_masks = []\n",
    "        with torch.inference_mode():\n",
    "            for id_idx in range(len(pass3_bboxes)):\n",
    "                if id_idx == 0:\n",
    "                    all_outputs = all_object_outputs\n",
    "                else:\n",
    "                    all_outputs = all_parts_outputs\n",
    "\n",
    "                think = pass3_thinks[id_idx]\n",
    "                \n",
    "                if len(pass3_bboxes[id_idx]) == 0:\n",
    "                    intersection = 0\n",
    "                    union = np.array(gt_mask_list[id_idx]).sum()\n",
    "                    bbox_iou = 0.0\n",
    "                    all_outputs.append({\n",
    "                        \"image_id\": filename,\n",
    "                        \"ann_id\": filename,\n",
    "                        \"seg_id\": seg_list[id_idx],\n",
    "                        \"think\": think,\n",
    "                        \"intersection\": int(intersection),\n",
    "                        \"union\": int(union),\n",
    "                        \"bbox_iou\": bbox_iou,\n",
    "                        \"visualization_path\": None\n",
    "                    })\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    segmentation_model.set_image(image)\n",
    "                    mask_all = np.zeros((original_height, original_width), dtype=bool)\n",
    "                except Exception as e:\n",
    "                    print(\"Set image error: \", e, filename)\n",
    "                    # skip this because the image or mask is not correct\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    bboxes = pass3_bboxes[id_idx]\n",
    "                    points = pass3_points[id_idx]\n",
    "                    for bbox, point in zip(bboxes, points):\n",
    "                        masks, scores, _ = segmentation_model.predict(\n",
    "                            point_coords=[point],\n",
    "                            point_labels=[1],\n",
    "                            box=bbox\n",
    "                        )\n",
    "                        sorted_ind = np.argsort(scores)[::-1]\n",
    "                        masks = masks[sorted_ind]\n",
    "                        mask = masks[0].astype(bool)\n",
    "                        mask_all = np.logical_or(mask_all, mask)\n",
    "                    gt_mask = np.array(gt_mask_list[id_idx])\n",
    "                except Exception as e:\n",
    "                    print(\"Segmentation error: \", e, filename)\n",
    "                    # skip this because the image or mask is not correct\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    intersection, union = compute_iou(mask_all, gt_mask)\n",
    "                except Exception as e:\n",
    "                    print(\"Image error: \", e)\n",
    "                    # skip this because the image or mask is not correct\n",
    "                    continue\n",
    "\n",
    "                pass3_predicted_masks.append(mask_all)\n",
    "\n",
    "                all_outputs.append({\n",
    "                    \"image_id\": filename,\n",
    "                    \"ann_id\": filename,\n",
    "                    \"seg_id\": seg_list[id_idx],\n",
    "                    \"think\": think,\n",
    "                    \"intersection\": int(intersection),\n",
    "                    \"union\": int(union),\n",
    "                    \"bbox_iou\": 0,\n",
    "                    \"visualization_path\": None\n",
    "                })\n",
    "\n",
    "        # print(f\"pass3_json_strings: {pass3_json_strings}\")\n",
    "        # print(f\"pass3_bboxes: {pass3_bboxes}\")  \n",
    "        # print(f\"pass3_points: {pass3_points}\")\n",
    "        # print(f\"pass3_thinks: {pass3_thinks}\")\n",
    "\n",
    "        ################ Visualization and analysis code ################\n",
    "        # For Pass 2:\n",
    "        ## We want to check if the captions generated match the object/part names that we passed. \n",
    "        ## We also want to see if the bounding boxes that we passed match the bounding boxes generated in the caption output \n",
    "        ## We also want to visualize the bounding boxes on the image to see if the generated labels/captions are correct.\n",
    "        ## We want to visualize the filtered bboxes to make sure we are providig only the relevant bboxes to Pass 3 -- self critique and bbox refinement\n",
    "\n",
    "        # For pass 3:\n",
    "        ## We want to add one axis to the visualization that shows the predicted boxes from pass 3\n",
    "        ## We want to add another axis that shows the predicted masks from pass 3\n",
    "        ## We want to add another axis that shows the ground truth masks for comparison\n",
    "        ## We want to add the pass3 thinking process as a text box on the visualization\n",
    "\n",
    "        # visualize_bbox_comparison(image, pass1_json_strings, pass2_json_strings_cleaned, pass2_json_strings_filtered, pass3_bboxes, pass3_predicted_masks, gt_mask_list, pass3_thinks, seg_list, filename, x_factor, y_factor)\n",
    "\n",
    "        # End of inference using the LLM        \n",
    "        \n",
    "        # break # doing it for one object and its parts for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "73ccdf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_giou(results):\n",
    "    all_ious = []\n",
    "        \n",
    "    # process all items in each file\n",
    "    for item in results:\n",
    "        intersection = item['intersection']\n",
    "        union = item['union']\n",
    "        \n",
    "        # calculate IoU of each item\n",
    "        iou = intersection / union if union > 0 else 0\n",
    "        all_ious.append({\n",
    "            'image_id': item['image_id'],\n",
    "            'iou': iou\n",
    "        })\n",
    "            \n",
    "    # calculate gIoU\n",
    "    gIoU = np.mean([item['iou'] for item in all_ious])\n",
    "    # calculate cIoU\n",
    "\n",
    "    # print the results\n",
    "    print(f\"gIoU (average of per image IoU): {gIoU:.4f}\")\n",
    "\n",
    "    return gIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f05870c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gIoU (average of per image IoU): 0.8472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.8471710200294912)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_giou(all_object_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41fcfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segzero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
