{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "560c6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ksmehrab/AttentionGrounding/ModelPlaygrounds/SegZero/GitRepoLatest/Seg-Zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9ed0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksmehrab/miniconda/envs/segzero/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import argparse\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "import json\n",
    "import pdb\n",
    "\n",
    "import cv2\n",
    "from PIL import Image as PILImage\n",
    "import re\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "181abc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_args(args=None):\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--reasoning_model_path\", type=str, default=\"/home/ksmehrab/AttentionGrounding/ModelPlaygrounds/SegZero/GitRepoLatest/Seg-Zero/pretrained_models/VisionReasoner-7B\")\n",
    "#     parser.add_argument(\"--segmentation_model_path\", type=str, default=\"facebook/sam2-hiera-large\")\n",
    "#     parser.add_argument(\"--text\", type=str, default=\"What can I have if I'm thirsty?\")\n",
    "#     parser.add_argument(\"--image_path\", type=str, default=\"/home/ksmehrab/AttentionGrounding/ModelPlaygrounds/SegZero/GitRepoLatest/Seg-Zero/assets/food.webp\")\n",
    "#     parser.add_argument(\"--output_path\", type=str, default=\"./inference_scripts/test_output_multiobject.png\")\n",
    "#     return parser.parse_args(args)\n",
    "\n",
    "# args = parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605748a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration object, to replace args\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.reasoning_model_path = \"/home/ksmehrab/AttentionGrounding/ModelPlaygrounds/SegZero/GitRepoLatest/Seg-Zero/pretrained_models/VisionReasoner-7B\"\n",
    "        self.segmentation_model_path = \"facebook/sam2-hiera-large\"\n",
    "        self.image_path = \"/home/ksmehrab/AttentionGrounding/ModelPlaygrounds/SegZero/GitRepoLatest/Seg-Zero/assets/food.webp\"\n",
    "        self.output_path = \"./test_output.png\"\n",
    "\n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is to parse the output from default segzero  \n",
    "def extract_bbox_points_think(output_text, x_factor, y_factor):\n",
    "    json_match = re.search(r'<answer>\\s*(.*?)\\s*</answer>', output_text, re.DOTALL)\n",
    "    if json_match:\n",
    "        data = json.loads(json_match.group(1))\n",
    "        pred_bboxes = [[\n",
    "            int(item['bbox_2d'][0] * x_factor + 0.5),\n",
    "            int(item['bbox_2d'][1] * y_factor + 0.5),\n",
    "            int(item['bbox_2d'][2] * x_factor + 0.5),\n",
    "            int(item['bbox_2d'][3] * y_factor + 0.5)\n",
    "        ] for item in data]\n",
    "        pred_points = [[\n",
    "            int(item['point_2d'][0] * x_factor + 0.5),\n",
    "            int(item['point_2d'][1] * y_factor + 0.5)\n",
    "        ] for item in data]\n",
    "    \n",
    "    think_pattern = r'<think>([^<]+)</think>'\n",
    "    think_match = re.search(think_pattern, output_text)\n",
    "    think_text = \"\"\n",
    "    if think_match:\n",
    "        think_text = think_match.group(1)\n",
    "    \n",
    "    return pred_bboxes, pred_points, think_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9180782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New output parse for vrpart format \n",
    "import json \n",
    "import re \n",
    "def extract_information_vrpart(output_text, x_factor, y_factor):\n",
    "    # Extract think tag\n",
    "    think_pattern = r'<think>([^<]+)</think>'\n",
    "    think_match = re.search(think_pattern, output_text)\n",
    "    think_text = think_match.group(1).strip() if think_match else \"\"\n",
    "    \n",
    "    # Extract decide tag\n",
    "    decide_pattern = r'<decide>([^<]+)</decide>'\n",
    "    decide_match = re.search(decide_pattern, output_text)\n",
    "    decide_text = decide_match.group(1).strip() if decide_match else \"\"\n",
    "    \n",
    "    # Extract first_answer tag\n",
    "    first_answer_pattern = r'<first_answer>\\s*(.*?)\\s*</first_answer>'\n",
    "    first_answer_match = re.search(first_answer_pattern, output_text, re.DOTALL)\n",
    "    first_answer_text = first_answer_match.group(1).strip() if first_answer_match else \"\"\n",
    "    \n",
    "    # Extract criticism tag\n",
    "    criticism_pattern = r'<criticism>([^<]+)</criticism>'\n",
    "    criticism_match = re.search(criticism_pattern, output_text)\n",
    "    criticism_text = criticism_match.group(1).strip() if criticism_match else \"\"\n",
    "    \n",
    "    # Extract final_answer and parse bbox/points\n",
    "    final_answer_pattern = r'<final_answer>\\s*(.*?)\\s*</final_answer>'\n",
    "    final_answer_match = re.search(final_answer_pattern, output_text, re.DOTALL) \n",
    "    final_answer_text = final_answer_match.group(1).strip() if final_answer_match else \"\"\n",
    "\n",
    "    output_text_parsed = {\n",
    "        \"think\": think_text,\n",
    "        \"decide\": decide_text,\n",
    "        \"first_answer\": first_answer_text,\n",
    "        \"criticism\": criticism_text,\n",
    "        \"final_answer\": final_answer_text\n",
    "    }\n",
    "    \n",
    "    pred_bboxes = []\n",
    "    pred_points = []\n",
    "    \n",
    "    if final_answer_match:\n",
    "        data = json.loads(final_answer_match.group(1))\n",
    "        pred_bboxes = [[\n",
    "            int(item['bbox_2d'][0] * x_factor + 0.5),\n",
    "            int(item['bbox_2d'][1] * y_factor + 0.5),\n",
    "            int(item['bbox_2d'][2] * x_factor + 0.5),\n",
    "            int(item['bbox_2d'][3] * y_factor + 0.5)\n",
    "        ] for item in data]\n",
    "        pred_points = [[\n",
    "            int(item['point_2d'][0] * x_factor + 0.5),\n",
    "            int(item['point_2d'][1] * y_factor + 0.5)\n",
    "        ] for item in data]\n",
    "\n",
    "    return pred_bboxes, pred_points, output_text_parsed\n",
    "\n",
    "# test the vrpart output parser\n",
    "example_output = \"\"\"<think> The image depicts a suitcase with a telescopic handle extending from its top. The task is to identify and mark the telescopic handle. The handle appears to be a rigid, straight rod that extends upwards from the top of the suitcase.</think> \n",
    "\n",
    "<decide>I am finding an \"object part\"</decide> \n",
    "\n",
    "<first_answer>[{\"bbox_2d\":[406,9,658,196], \"point_2d\":[517,88]}]</first_answer> \n",
    "\n",
    "<criticism>The initial bounding box around the telescopic handle might be too large because it includes parts of the suitcase structure that are not relevant to the handle itself. A more accurate bounding box should only encompass the visible part of the handle, which is the vertical rod.</criticism> \n",
    "\n",
    "<final_answer>[{\"bbox_2d\":[407,10,658,197], \"point_2d\":[520,87]}]</final_answer>\n",
    "\"\"\"\n",
    "\n",
    "extract_information_vrpart(example_output, 1.0, 1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97eab7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(mask1, mask2):\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return intersection, union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0747baf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "#We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "reasoning_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    args.reasoning_model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f48600",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd8a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_model = SAM2ImagePredictor.from_pretrained(args.segmentation_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cad1659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(args.reasoning_model_path, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1dccc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_TEMPLATE = \\\n",
    "    \"Please find \\\"{Question}\\\" with bboxs and points.\" \\\n",
    "    \"Compare the difference between object(s) and find the most closely matched object(s).\" \\\n",
    "    \"Output the thinking process in <think> </think> and final answer in <answer> </answer> tags.\" \\\n",
    "    \"Output the bbox(es) and point(s) inside the interested object(s) in JSON format.\" \\\n",
    "    \"i.e., <think> thinking process here </think>\" \\\n",
    "    \"<answer>{Answer}</answer>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1045e810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already done on 0 images. Running on remaining 925...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:35<00:00, 27.53s/it]\n"
     ]
    }
   ],
   "source": [
    "## Code for getting pascalpart images and object/object parts \n",
    "sys.path.append('/home/ksmehrab/AttentionGrounding/Baselines/Models/GLAMM')\n",
    "sys.path.append('/home/ksmehrab/AttentionGrounding/Baselines/Datasets')\n",
    "sys.path.append('/home/ksmehrab/AttentionGrounding/Baselines')\n",
    "    \n",
    "from pascalpart import get_pascalpart_classes, get_pascalpart_masks\n",
    "from utils import read_txt_file, save_to_json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "pascal_image_dir = \"/data/Pascal_VOC_2012/VOCdevkit/VOC2012/JPEGImages\" # 17125 images\n",
    "annotations_path= \"/data/PartSegmentationDatasets/PascalPart/Annotations_Part\"\n",
    "val_filepath = \"/data/PartSegmentationDatasets/PascalPart/val.txt\" # 925 images. File contains just the file prefix. Add .jpg extension for images, and .mat extension for annotations\n",
    "val_filenames = read_txt_file(val_filepath)\n",
    "\n",
    "## Change \n",
    "save_dir = \"/data/VLMGroundingProject/BaselineResults/PascalPart/SegZero\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "already_done = [os.path.splitext(f)[0] for f in os.listdir(save_dir)]\n",
    "\n",
    "req_filenames = [f for f in val_filenames if f not in already_done]\n",
    "\n",
    "print(f'Already done on {len(already_done)} images. Running on remaining {len(req_filenames)}...')\n",
    "\n",
    "def combine_masks(masks):\n",
    "    combined_mask = masks[0]\n",
    "    for i in range(1, len(masks)):\n",
    "        combined_mask = combined_mask | masks[i]\n",
    "    return combined_mask\n",
    "\n",
    "all_object_outputs = []\n",
    "all_parts_outputs = []\n",
    "for filename in tqdm(req_filenames[:10]):\n",
    "    img_filepath = os.path.join(pascal_image_dir, filename+'.jpg')\n",
    "\n",
    "    image = PILImage.open(img_filepath)\n",
    "    image = image.convert(\"RGB\")\n",
    "    original_width, original_height = image.size\n",
    "    resize_size = 840\n",
    "    x_factor, y_factor = original_width/resize_size, original_height/resize_size\n",
    "\n",
    "    annot_filename = filename + '.mat'\n",
    "    anno_dict = get_pascalpart_masks(annot_filename, annotations_path, images_path=pascal_image_dir)\n",
    "    \n",
    "    # classes_to_detect = get_pascalpart_classes(annot_filename, annotations_path) # This is a dictionary in this format {object: [list of parts]}\n",
    "    # print(classes_to_detect)\n",
    "    for obj_name, anno in anno_dict.items():\n",
    "        obj_masks = anno['object_maps']\n",
    "        obj_mask = combine_masks(obj_masks)\n",
    "\n",
    "        gt_mask_list = [obj_mask]\n",
    "        seg_list = [obj_name]\n",
    "\n",
    "        parts_masks = anno['parts']\n",
    "        # print(parts_masks.keys())\n",
    "        for part_name, masks in parts_masks.items():\n",
    "            part_mask = combine_masks(masks)\n",
    "            gt_mask_list.append(part_mask)\n",
    "            part_full_name = obj_name + \"'s \" + part_name\n",
    "            seg_list.append(part_full_name)\n",
    "\n",
    "        messages = []\n",
    "        for args_text in seg_list:\n",
    "            message = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\", \n",
    "                    \"image\": image.resize((resize_size, resize_size), PILImage.BILINEAR)\n",
    "                },\n",
    "                {   \n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": QUESTION_TEMPLATE.format(\n",
    "                        Question=args_text.lower().strip(\".\"),\n",
    "                        Answer=\"[{\\\"bbox_2d\\\": [10,100,200,210], \\\"point_2d\\\": [30,110]}, {\\\"bbox_2d\\\": [225,296,706,786], \\\"point_2d\\\": [302,410]}]\"\n",
    "                    )    \n",
    "                }\n",
    "            ]\n",
    "            }]\n",
    "            messages.append(message)\n",
    "        \n",
    "        # Preparation for inference\n",
    "        text = [processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages]\n",
    "    \n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    " \n",
    "        inputs = processor(\n",
    "            text=text,\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "\n",
    "        # Inference: Generation of the output\n",
    "        # add torch inference mode and autocast here for faster inference\n",
    "        generated_ids = reasoning_model.generate(**inputs, use_cache=True, max_new_tokens=1024, do_sample=False)\n",
    "\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        batch_output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        with torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            for id_idx in range(len(batch_output_text)):\n",
    "                if id_idx == 0:\n",
    "                    all_outputs = all_object_outputs\n",
    "                else:\n",
    "                    all_outputs = all_parts_outputs\n",
    "                try:\n",
    "                    bboxes, points, think = extract_bbox_points_think(\n",
    "                                            batch_output_text[id_idx], \n",
    "                                            original_width/resize_size, \n",
    "                                            original_height/resize_size\n",
    "                                        )\n",
    "                except Exception as e:\n",
    "                    # add penalty in this situation\n",
    "                    print(\"Reasoning error: \", e, \"Text: \", batch_output_text[id_idx], \"Image: \", filename, \"Seg: \", seg_list[id_idx])\n",
    "                    think = \"\"\n",
    "                    intersection = 0\n",
    "                    union = np.array(gt_mask_list[id_idx]).sum()\n",
    "                    bbox_iou = 0.0\n",
    "                    all_outputs.append({\n",
    "                        \"image_id\": filename,\n",
    "                        \"ann_id\": filename,\n",
    "                        \"seg_id\": seg_list[id_idx],\n",
    "                        \"think\": think,\n",
    "                        \"intersection\": int(intersection),\n",
    "                        \"union\": int(union),\n",
    "                        \"bbox_iou\": bbox_iou,\n",
    "                        \"visualization_path\": None\n",
    "                    })\n",
    "                    continue\n",
    "                try:\n",
    "                    segmentation_model.set_image(image)\n",
    "                    mask_all = np.zeros((original_height, original_width), dtype=bool)\n",
    "                except Exception as e:\n",
    "                    print(\"Set image error: \", e, filename)\n",
    "                    # skip this because the image or mask is not correct\n",
    "                    continue\n",
    "                try:\n",
    "                    for bbox, point in zip(bboxes, points):\n",
    "                        masks, scores, _ = segmentation_model.predict(\n",
    "                            point_coords=[point],\n",
    "                            point_labels=[1],\n",
    "                            box=bbox\n",
    "                        )\n",
    "                        sorted_ind = np.argsort(scores)[::-1]\n",
    "                        masks = masks[sorted_ind]\n",
    "                        mask = masks[0].astype(bool)\n",
    "                        mask_all = np.logical_or(mask_all, mask)\n",
    "                    gt_mask = np.array(gt_mask_list[id_idx])\n",
    "                except Exception as e:\n",
    "                    print(\"Segmentation error: \", e, filename)\n",
    "                    # skip this because the image or mask is not correct\n",
    "                    continue\n",
    "                try:\n",
    "                    intersection, union = compute_iou(mask_all, gt_mask)\n",
    "                except Exception as e:\n",
    "                    print(\"Image error: \", e)\n",
    "                    # skip this because the image or mask is not correct\n",
    "                    continue \n",
    "                \n",
    "                # save mask_all in save_dir/filename/mask_save_filename \n",
    "                mask_save_dir = os.path.join(save_dir, filename)\n",
    "                os.makedirs(mask_save_dir, exist_ok=True)\n",
    "                mask_save_filename = seg_list[id_idx].replace(\"'s\", \"\").replace(\" \", \"_\") + \".npy\"\n",
    "                mask_save_filepath = os.path.join(mask_save_dir, mask_save_filename)\n",
    "                np.save(mask_save_filepath, mask_all)\n",
    "\n",
    "\n",
    "                # Create visualization\n",
    "                import matplotlib.pyplot as plt\n",
    "\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "                # Original image\n",
    "                axes[0].imshow(image)\n",
    "                axes[0].set_title('Original Image')\n",
    "                axes[0].axis('off')\n",
    "\n",
    "                # Predicted mask overlay\n",
    "                axes[1].imshow(image)\n",
    "                predicted_overlay = np.zeros((mask_all.shape[0], mask_all.shape[1], 4))\n",
    "                predicted_overlay[mask_all, 0] = 1.0  # Red color for prediction\n",
    "                predicted_overlay[mask_all, 3] = 0.5  # Alpha transparency\n",
    "                axes[1].imshow(predicted_overlay)\n",
    "                axes[1].set_title(f'Predicted: {seg_list[id_idx]}')\n",
    "                axes[1].axis('off')\n",
    "\n",
    "                # Ground truth mask overlay\n",
    "                axes[2].imshow(image)\n",
    "                gt_overlay = np.zeros((gt_mask.shape[0], gt_mask.shape[1], 4))\n",
    "                gt_overlay[gt_mask.astype(bool), 1] = 1.0  # Green color for ground truth\n",
    "                gt_overlay[gt_mask.astype(bool), 3] = 0.5  # Alpha transparency\n",
    "                axes[2].imshow(gt_overlay)\n",
    "                axes[2].set_title(f'Ground Truth: {seg_list[id_idx]}')\n",
    "                axes[2].axis('off')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                visualization_path = os.path.join(mask_save_dir, f\"{seg_list[id_idx].replace('\\'s', '').replace(' ', '_')}_visualization.png\")\n",
    "                plt.savefig(visualization_path)\n",
    "                # plt.show()\n",
    "                plt.close()\n",
    "                \n",
    "                all_outputs.append({\n",
    "                    \"image_id\": filename,\n",
    "                    \"ann_id\": filename,\n",
    "                    \"seg_id\": seg_list[id_idx],\n",
    "                    \"think\": think,\n",
    "                    \"intersection\": int(intersection),\n",
    "                    \"union\": int(union),\n",
    "                    \"bbox_iou\": 0,\n",
    "                    \"visualization_path\": visualization_path\n",
    "                })\n",
    "        \n",
    "        # clean GPU memory\n",
    "        del inputs, generated_ids, generated_ids_trimmed\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8446aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(os.path.join(\"objects_results.json\"), all_object_outputs)\n",
    "save_to_json(os.path.join(\"parts_results.json\"), all_parts_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276e6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_giou(file_paths):\n",
    "    if isinstance(file_paths, str):\n",
    "        with open(file_paths, 'r', encoding='utf-8') as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        # if list, combine all results\n",
    "        results = []\n",
    "        for fp in file_paths:\n",
    "            with open(fp, 'r', encoding='utf-8') as f:\n",
    "                results.extend(json.load(f))\n",
    "                \n",
    "    print(len(results))\n",
    "    all_ious = []\n",
    "\n",
    "    # process all items in each file\n",
    "    for item in results:\n",
    "        intersection = item['intersection']\n",
    "        union = item['union']\n",
    "        \n",
    "        # calculate IoU of each item\n",
    "        iou = intersection / union if union > 0 else 0\n",
    "        all_ious.append({\n",
    "            # 'image_id': item['image_id'],\n",
    "            'iou': iou\n",
    "        })\n",
    "            \n",
    "    # calculate gIoU\n",
    "    gIoU = np.mean([item['iou'] for item in all_ious])\n",
    "    # calculate cIoU\n",
    "\n",
    "    # print the results\n",
    "    print(f\"gIoU (average of per image IoU): {gIoU:.4f}\")\n",
    "\n",
    "    return gIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b434d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "gIoU (average of per image IoU): 0.7167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.7167356149856086)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_giou([f\"/data/VLMGroundingProject/BaselineResults/InstructPart/SAM3Text/parts_results_{chunk_id}.json\" for chunk_id in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43f6f162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1077\n",
      "gIoU (average of per image IoU): 0.8532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.8532021906003255)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute_giou([f\"/data/VLMGroundingProject/BaselineResults/PascalPart/SAM3/parts_results_{chunk_id}.json\" for chunk_id in range(4)])\n",
    "compute_giou([f\"/data/VLMGroundingProject/BaselineResults/PascalPart/SAM3/objects_results_{chunk_id}.json\" for chunk_id in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "813ca226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "gIoU (average of per image IoU): 0.7686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.768567738378893)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_giou(\"/data/VLMGroundingProject/BaselineResults/InstructPart/VRPart2_OneEpoch/parts_results_3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eec86b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4110\n",
      "gIoU (average of per image IoU): 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_giou(\"/data/VLMGroundingProject/BaselineResults/PascalPart/VRPart2/parts_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pascalpart:\n",
    "# vr: objects: 86.68, parts: 27.44\n",
    "# vrip: objects: 85.19, parts: 28.32 \n",
    "# vrpart: objects: 85.02, parts: 32.69 \n",
    "# zs qwen2.5 on our prompt: \n",
    "# vrpart2_nocontainment (2 epochs): objects: 85.39 , parts: 34.94\n",
    "# vrpart2 + containment + compactness (2 epochs): objects: 83.29 , parts: 33.69\n",
    "# sam3: objects: 85.32, parts: 33.05\n",
    "\n",
    "# instructpart: \n",
    "# vr: 59.38 \n",
    "# vr just trained on ip: 62.33\n",
    "# vr trained on ip and our reward: 67.14 \n",
    "# zs qwen2.5 on our prompt:\n",
    "# vrpart2_nocontainment (2 epochs) (still using the object-aware formulation, just not adding a reward for ensuring part is within object): 73.63\n",
    "# vrpart2 + containment + compactness (2 epochs): 72.24 (!) # maybe compactness is not helping? part within object should generally help \n",
    "# vrpart + containment + compactness (1 epoch): 69.64 \n",
    "# sam3: 71.67\n",
    "\n",
    "\n",
    "# partimagenet:\n",
    "# vr: parts: 45.59\n",
    "# vr just trained on ip: \n",
    "# vr trained on ip and our reward: 50.26\n",
    "# vrpart2_nocontainment (2 epochs): 53.00\n",
    "# vrpart2 + containment + compactness (2 epochs):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segzero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
